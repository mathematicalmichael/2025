3:I[4707,[],""]
6:I[6423,[],""]
7:I[2798,["185","static/chunks/app/layout-c0991f7d76273a13.js"],"ThemeProvider"]
8:I[9734,["185","static/chunks/app/layout-c0991f7d76273a13.js"],"DynamicFavicon"]
9:I[8291,["185","static/chunks/app/layout-c0991f7d76273a13.js"],"Analytics"]
4:["month","1","d"]
5:["day","9","d"]
0:["VQoYmXv-G56ZgJRRzJJFi",[[["",{"children":[["month","1","d"],{"children":[["day","9","d"],{"children":["__PAGE__?{\"month\":\"1\",\"day\":\"9\"}",{}]}]}]},"$undefined","$undefined",true],["",{"children":[["month","1","d"],{"children":[["day","9","d"],{"children":["__PAGE__",{},[["$L1","$L2",null],null],null]},[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children","$5","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[null,["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children","$4","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[[[["$","link","0",{"rel":"stylesheet","href":"/2025/_next/static/css/df8c932e539912be.css","precedence":"next","crossOrigin":"$undefined"}]],["$","html",null,{"lang":"en","suppressHydrationWarning":true,"className":"h-full","children":["$","body",null,{"className":"min-h-full bg-white dark:bg-black","children":[["$","$L7",null,{"children":["$","div",null,{"className":"flex flex-col min-h-full","children":[["$","$L8",null,{}],["$","div",null,{"className":"flex-1 w-full","children":["$","$L3",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[]}]}],["$","footer",null,{"className":"text-center text-sm text-gray-600 dark:text-gray-400 py-4 bg-white dark:bg-black","children":["© ",2025," Michael Pilosov. All rights reserved."]}]]}]}],["$","$L9",null,{}]]}]}]],null],null],["$La",null]]]]
b:I[2972,["450","static/chunks/450-fc8431448c3438bc.js","878","static/chunks/878-5d3156bd3161bc5d.js","160","static/chunks/app/%5Bmonth%5D/%5Bday%5D/page-475f15d3cd396c50.js"],""]
c:I[1190,["450","static/chunks/450-fc8431448c3438bc.js","878","static/chunks/878-5d3156bd3161bc5d.js","160","static/chunks/app/%5Bmonth%5D/%5Bday%5D/page-475f15d3cd396c50.js"],"ThemeToggle"]
d:I[5878,["450","static/chunks/450-fc8431448c3438bc.js","878","static/chunks/878-5d3156bd3161bc5d.js","160","static/chunks/app/%5Bmonth%5D/%5Bday%5D/page-475f15d3cd396c50.js"],"Image"]
2:["$","div",null,{"className":"min-h-screen bg-white dark:bg-black text-black dark:text-white","children":[["$","header",null,{"className":"flex justify-between items-center mb-8 px-4","children":[["$","$Lb",null,{"href":"/","className":"text-2xl font-bold hover:underline","children":"2025 (12 Events)"}],["$","div",null,{"className":"flex items-center","children":[["$","$Lb",null,{"href":"/about","className":"hover:underline mr-4","children":"About"}],["$","$Lc",null,{}]]}]]}],["$","nav",null,{"className":"flex justify-between w-full mb-8 text-3xl font-bold text-gray-600 dark:text-gray-400 px-4","children":[["$","div",null,{"className":"flex-1 flex justify-start","children":["$","$Lb",null,{"href":"/1/8","className":"hover:underline","children":"←"}]}],["$","div",null,{"className":"flex-1 flex justify-end","children":["$","$Lb",null,{"href":"/1/10","className":"hover:underline","children":"→"}]}]]}],["$","main",null,{"children":["$","div",null,{"className":"max-w-content mx-auto px-4","children":["$","article",null,{"children":[["$","h1",null,{"className":"text-3xl font-bold mb-2","children":"Explorations in Embedding Sensitivity to Initialization"}],["$","p",null,{"className":"text-gray-600 dark:text-gray-400 mb-4","children":"2025-01-09"}],["$","div",null,{"className":"prose dark:prose-invert mb-8 [&>p]:mb-4 [&_a]:underline [&_a:hover]:text-gray-600 dark:[&_a:hover]:text-gray-300 [&>ul]:list-disc [&>ul]:pl-6 [&>ul]:mb-4 [&>ul>li]:pl-2","children":[["$","p","0",{"children":["When reviewing my embedding scripts in preparing new images, I found that the ",["$","code","1",{"className":"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm","children":"nomic-embed-text-v1.5"}]," model card contained incorrect code snippets, which resulted in me using the ",["$","code","3",{"className":"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm","children":"v1"}]," version of the model.\nThe results shown prior (and below) are made using the earlier model."]}],"\n",["$","p","2",{"children":"Below, I am only changing the seed value for the dimension-reduction model, from 21 to 42 (original, I know).\nI also continue using the Gondry “GO” video frame results, mostly because they’re still on-disk and I want an apples-to-apples comparison in short time."}],"\n",["$","hr","4",{"children":"$undefined"}],"\n",["$","p","6",{"children":"\n"}],["$","div","7",{"className":"plotly-container","children":["\n        ",["$","iframe","1",{"src":"https://cdn.math.computer/2025/1/9/dataXdata_380.html","loading":"lazy","title":"Plotly visualization","children":"$undefined"}],"\n        ",["$","figcaption","3",{"className":"text-center text-lg text-gray-600 dark:text-gray-400","children":"Reduction done on video frames; seed 42"}],"\n      "]}],["$","p","8",{"children":"$undefined"}],"\n",["$","p","10",{"children":"\n"}],["$","div","11",{"className":"flex justify-center","children":["\n        ",["$","a","1",{"href":"https://cdn.math.computer/2025/1/9/dataXdata_950.html?test","className":"inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors","children":"\n          Full screen\n        "}],"\n      "]}],["$","p","12",{"children":"$undefined"}],"\n",["$","p","14",{"children":"The result above basically looks like a mild rotation and slight warping of the previous result (reproduced below)"}],"\n",["$","p","16",{"children":"\n"}],["$","div","17",{"className":"plotly-container","children":["\n        ",["$","iframe","1",{"src":"https://cdn.math.computer/v/kosmos2/go/dataXdata_380_v0.html","loading":"lazy","title":"Plotly visualization","children":"$undefined"}],"\n        ",["$","figcaption","3",{"className":"text-center text-lg text-gray-600 dark:text-gray-400","children":"Seed 21"}],"\n      "]}],["$","p","18",{"children":"$undefined"}],"\n",["$","p","20",{"children":"\n"}],["$","div","21",{"className":"flex justify-center","children":["\n        ",["$","a","1",{"href":"https://cdn.math.computer/v/kosmos2/go/dataXdata_950_v0.html?test","className":"inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors","children":"\n          Full screen\n        "}],"\n      "]}],["$","p","22",{"children":"$undefined"}],"\n",["$","hr","24",{"children":"$undefined"}],"\n",["$","p","26",{"children":["By contrast, the significantly larger dataset of human-captioned images led to a ",["$","em","1",{"children":"much"}]," more stable embedding space result, with only minor variations in structure."]}],"\n",["$","p","28",{"children":"\n"}],["$","div","29",{"className":"plotly-container","children":["\n        ",["$","iframe","1",{"src":"https://cdn.math.computer/2025/1/9/dataXtrain_380.html","loading":"lazy","title":"Plotly visualization","children":"$undefined"}],"\n        ",["$","figcaption","3",{"className":"text-center text-lg text-gray-600 dark:text-gray-400","children":"Reduction done on human-labeled captions; seed 42"}],"\n      "]}],["$","p","30",{"children":"$undefined"}],"\n",["$","p","32",{"children":"\n"}],["$","div","33",{"className":"flex justify-center","children":["\n        ",["$","a","1",{"href":"https://cdn.math.computer/2025/1/9/dataXtrain_950.html?test","className":"inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors","children":"\n          Full screen\n        "}],"\n      "]}],["$","p","34",{"children":"$undefined"}],"\n",["$","p","36",{"children":"\n"}],["$","div","37",{"className":"plotly-container","children":["\n        ",["$","iframe","1",{"src":"https://cdn.math.computer/v/kosmos2/go/dataXtrain_380_v0.html","loading":"lazy","title":"Plotly visualization","children":"$undefined"}],"\n        ",["$","figcaption","3",{"className":"text-center text-lg text-gray-600 dark:text-gray-400","children":"Seed 21"}],"\n      "]}],["$","p","38",{"children":"$undefined"}],"\n",["$","p","40",{"children":"\n"}],["$","div","41",{"className":"flex justify-center","children":["\n        ",["$","a","1",{"href":"https://cdn.math.computer/v/kosmos2/go/dataXtrain_950_v0.html?test","className":"inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors","children":"\n          Full screen\n        "}],"\n      "]}],["$","p","42",{"children":"$undefined"}],"\n",["$","p","44",{"children":["This is remarkably good behavior being exhibited by ",["$","code","1",{"className":"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm","children":"PaCMAP"}],", which I have not seen to be the case with ",["$","code","3",{"className":"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm","children":"UMAP"}]," which in my prior experience has a lot more variation in the results on large datasets."]}],"\n",["$","p","46",{"children":["More reason to keep going with the ",["$","code","1",{"className":"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm","children":"fit"}]," on the large dataset, and ",["$","code","3",{"className":"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm","children":"transform"}]," smaller ones, when possible."]}],"\n",["$","p","48",{"children":"Both results are aesthetically interesting, but I’ve learned that sensitivity in the final embedding is largely a function of sample size."}],"\n",["$","h2","50",{"children":"Upgrading embedding models"}],"\n",["$","p","52",{"children":"Going back to seed 21, I re-ran the pipeline with only the embedding model changing (an unexpected experiment)."}],"\n",["$","p","54",{"children":"\n"}],["$","div","55",{"className":"plotly-container","children":["\n        ",["$","iframe","1",{"src":"https://cdn.math.computer/2025/1/9/dataXdata_380_v0.html","loading":"lazy","title":"Plotly visualization","children":"$undefined"}],"\n        ",["$","figcaption","3",{"className":"text-center text-lg text-gray-600 dark:text-gray-400","children":"Reduction done on video frames; seed 21"}],"\n      "]}],["$","p","56",{"children":"$undefined"}],"\n",["$","p","58",{"children":"\n"}],["$","div","59",{"className":"flex justify-center","children":["\n        ",["$","a","1",{"href":"https://cdn.math.computer/2025/1/9/dataXdata_950_v0.html","className":"inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors","children":"\n          Full screen\n        "}],"\n      "]}],["$","p","60",{"children":"$undefined"}],"\n",["$","p","62",{"children":"The embedding where only the frames were used is not too different, but does exhibit slightly different structure (if not similar aeshethics)."}],"\n",["$","p","64",{"children":"However, when I perform a large-scale training first in order to do the embedding, there was a much more dramatic change:"}],"\n",["$","p","66",{"children":"\n"}],["$","div","67",{"className":"plotly-container","children":["\n        ",["$","iframe","1",{"src":"https://cdn.math.computer/2025/1/9/dataXtrain_380_v0.html","loading":"lazy","title":"Plotly visualization","children":"$undefined"}],"\n        ",["$","figcaption","3",{"className":"text-center text-lg text-gray-600 dark:text-gray-400","children":"Reduction done on human-labeled captions; seed 21"}],"\n      "]}],["$","p","68",{"children":"$undefined"}],"\n",["$","p","70",{"children":"\n"}],["$","div","71",{"className":"flex justify-center","children":["\n        ",["$","a","1",{"href":"https://cdn.math.computer/2025/1/9/dataXtrain_950_v0.html","className":"inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors","children":"\n          Full screen\n        "}],"\n      "]}],["$","p","72",{"children":"$undefined"}],"\n",["$","p","74",{"children":["I really ",["$","em","1",{"children":"REALLY"}]," like what I’m seeing here now.\nThere seems to be more logical continuity with respect to the locations of “women” vs “woman” and “groups” being between that and “men” / “man”.\nPoints are clustered far more closely together, which I am sure will result in a lower distance traveled than anything I’ve done prior."]}],"\n",["$","p","76",{"children":["This says to me that the ",["$","code","1",{"className":"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm","children":"v1.5"}]," version of the ",["$","code","3",{"className":"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm","children":"nomic-embed"}]," model is a substantial improvement (at least to the eyeball) over ",["$","code","5",{"className":"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm","children":"v1"}],".\nThere is probably a lot more exploring to do with respect to better performing sentence-embedding models (or just text-embedding in general)."]}],"\n",["$","hr","78",{"children":"$undefined"}],"\n",["$","p","80",{"children":"And for good measure, the training dataset itself:"}],"\n",["$","p","82",{"children":"\n"}],["$","figure","83",{"children":["\n        ",["$","$Ld","1",{"src":"https://cdn.math.computer/2025/1/9/trainXtrain_002.png?test","alt":"","width":800,"height":600,"className":"w-auto h-auto rounded-md mx-auto min-w-[min(640px,100%)]","sizes":"800px","quality":78}],"\n        ",["$","figcaption","3",{"className":"text-center text-sm text-gray-600 dark:text-gray-400","children":"Training data visualized with `nomic-embed-v1.5`"}],"\n      "]}],["$","p","84",{"children":"$undefined"}],"\n",["$","p","86",{"children":"\n"}],["$","div","87",{"className":"flex justify-center","children":["\n        ",["$","a","1",{"href":"https://cdn.math.computer/2025/1/9/trainXtrain_950_v0.html","className":"inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors","children":"\n          Full screen\n        "}],"\n      "]}],["$","p","88",{"children":"$undefined"}],"\n",["$","p","90",{"children":"Aesthetically (to my mind), this is one of the more pleasing images to have come out of this exercise so far."}],"\n",["$","p","92",{"children":"I happen to know that there’s a complimentary dataset available from the same source of images that I pulled the captions.\nThis other dataset is machine-generated labels (much like Kosmos-2 is doing).\nI would absolutely love to see the result of comparing human to computer labeled images using this pipeline.\nCould I learn to identify machine-generated descriptions of imagery apart from human ones?"}],"\n",["$","hr","94",{"children":"$undefined"}],"\n",["$","h2","96",{"children":"§ coda"}],"\n",["$","p","98",{"children":["I really loved how my ",["$","code","1",{"className":"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm","children":"Makefile"}]," helped prevent careless mistakes with today’s workflow.\nAs I was switching out seeds and text-embedding models, I could rest assured that the files I was uploading to my content server were not stale.\nHuman error is easy to encounter; I bounce between writing these posts while waiting for images to generate.\nWhile generating the last image, I forgot that a certain step needed to be run before an image could generate - so it was wonderful that ",["$","code","3",{"className":"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm","children":"make"}]," detected the need to re-run the step on my behalf.\nThe less I need to think about cache-invalidation, the better."]}],"\n",["$","h2","100",{"children":"§§ coda"}],"\n",["$","p","102",{"children":"I came across a new dimension-reduction algorithm as I was working on some open-source stuff and ran it on the training data."}],"\n",["$","p","104",{"children":"The result was visually much more compelling than anything else, and exhibits far more separation between clusters of semantically similar captions."}],"\n",["$","p","106",{"children":"\n"}],["$","figure","107",{"children":["\n        ",["$","$Ld","1",{"src":"https://cdn.math.computer/2025/1/9/trainXtrain.png?test","alt":"","width":800,"height":600,"className":"w-auto h-auto rounded-md mx-auto min-w-[min(640px,100%)]","sizes":"800px","quality":78}],"\n        ",["$","figcaption","3",{"className":"text-center text-sm text-gray-600 dark:text-gray-400","children":"Training data visualized with `nomic-embed-v1.5` and a new dimension-reduction method"}],"\n      "]}],["$","p","108",{"children":"$undefined"}],"\n",["$","p","110",{"children":"\n"}],["$","div","111",{"className":"flex justify-center","children":["\n        ",["$","a","1",{"href":"https://cdn.math.computer/2025/1/9/trainXtrain_950.html","className":"inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors","children":"\n          Full screen\n        "}],"\n      "]}],["$","p","112",{"children":"$undefined"}],"\n",["$","p","114",{"children":"The library was in very early stages of development, but now I have motivation to contribute to cleaning it up, as it seems to be doing really well with balancing repulsion and attraction of points."}],"\n"]}],["$","div",null,{"className":"grid grid-cols-1 md:grid-cols-2 gap-4","children":[]}]]}]}]}],["$","nav",null,{"className":"flex justify-between w-full mt-8 mb-4 text-3xl font-bold text-gray-600 dark:text-gray-400 px-4","children":[["$","div",null,{"className":"flex-1 flex justify-start","children":["$","$Lb",null,{"href":"/1/8","className":"hover:underline","children":"←"}]}],["$","div",null,{"className":"flex-1 flex justify-end","children":["$","$Lb",null,{"href":"/1/10","className":"hover:underline","children":"→"}]}]]}]]}]
a:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"MathematicalMichaelx2025"}],["$","meta","3",{"name":"description","content":"Digital Art Log for Dr. Michael Pilosov"}],["$","link","4",{"rel":"icon","href":"/2025/favicon.ico","type":"image/x-icon","sizes":"48x48"}]]
1:null
