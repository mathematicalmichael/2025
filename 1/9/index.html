<!DOCTYPE html><html lang="en" class="h-full"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/2025/_next/static/css/df8c932e539912be.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/2025/_next/static/chunks/webpack-b2ce682491b35475.js"/><script src="/2025/_next/static/chunks/fd9d1056-a86d52f6b747304e.js" async=""></script><script src="/2025/_next/static/chunks/117-003a6b1a715a95e6.js" async=""></script><script src="/2025/_next/static/chunks/main-app-3c13ae280b9991a1.js" async=""></script><script src="/2025/_next/static/chunks/app/layout-c0991f7d76273a13.js" async=""></script><script src="/2025/_next/static/chunks/450-fc8431448c3438bc.js" async=""></script><script src="/2025/_next/static/chunks/878-5d3156bd3161bc5d.js" async=""></script><script src="/2025/_next/static/chunks/app/%5Bmonth%5D/%5Bday%5D/page-475f15d3cd396c50.js" async=""></script><title>MathematicalMichaelx2025</title><meta name="description" content="Digital Art Log for Dr. Michael Pilosov"/><link rel="icon" href="/2025/favicon.ico" type="image/x-icon" sizes="48x48"/><script src="/2025/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="min-h-full bg-white dark:bg-black"><script>((e,t,r,n,l,o,a,u)=>{let i=document.documentElement,c=["light","dark"];function d(t){(Array.isArray(e)?e:[e]).forEach(e=>{let r="class"===e,n=r&&o?l.map(e=>o[e]||e):l;r?(i.classList.remove(...n),i.classList.add(t)):i.setAttribute(e,t)}),u&&c.includes(t)&&(i.style.colorScheme=t)}if(n)d(n);else try{let e=localStorage.getItem(t)||r,n=a&&"system"===e?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":e;d(n)}catch(e){}})("class","theme","system",null,["light","dark"],null,true,true)</script><div class="flex flex-col min-h-full"><div class="flex-1 w-full"><div class="min-h-screen bg-white dark:bg-black text-black dark:text-white"><header class="flex justify-between items-center mb-8 px-4"><a class="text-2xl font-bold hover:underline" href="/2025/">2025 (12 Events)</a><div class="flex items-center"><a class="hover:underline mr-4" href="/2025/about/">About</a><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 hover:text-accent-foreground h-9 hover:bg-transparent p-0" disabled=""><div class="h-5 w-5"></div><span class="sr-only">Loading theme toggle</span></button></div></header><nav class="flex justify-between w-full mb-8 text-3xl font-bold text-gray-600 dark:text-gray-400 px-4"><div class="flex-1 flex justify-start"><a class="hover:underline" href="/2025/1/8/">←</a></div><div class="flex-1 flex justify-end"><a class="hover:underline" href="/2025/1/10/">→</a></div></nav><main><div class="max-w-content mx-auto px-4"><article><h1 class="text-3xl font-bold mb-2">Explorations in Embedding Sensitivity to Initialization</h1><p class="text-gray-600 dark:text-gray-400 mb-4">2025-01-09</p><div class="prose dark:prose-invert mb-8 [&amp;&gt;p]:mb-4 [&amp;_a]:underline [&amp;_a:hover]:text-gray-600 dark:[&amp;_a:hover]:text-gray-300 [&amp;&gt;ul]:list-disc [&amp;&gt;ul]:pl-6 [&amp;&gt;ul]:mb-4 [&amp;&gt;ul&gt;li]:pl-2"><p>When reviewing my embedding scripts in preparing new images, I found that the <code class="bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm">nomic-embed-text-v1.5</code> model card contained incorrect code snippets, which resulted in me using the <code class="bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm">v1</code> version of the model.
The results shown prior (and below) are made using the earlier model.</p>
<p>Below, I am only changing the seed value for the dimension-reduction model, from 21 to 42 (original, I know).
I also continue using the Gondry “GO” video frame results, mostly because they’re still on-disk and I want an apples-to-apples comparison in short time.</p>
<hr/>
<p>
</p><div class="plotly-container">
        <iframe src="https://cdn.math.computer/2025/1/9/dataXdata_380.html" loading="lazy" title="Plotly visualization"></iframe>
        <figcaption class="text-center text-lg text-gray-600 dark:text-gray-400">Reduction done on video frames; seed 42</figcaption>
      </div><p></p>
<p>
</p><div class="flex justify-center">
        <a href="https://cdn.math.computer/2025/1/9/dataXdata_950.html?test" class="inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors">
          Full screen
        </a>
      </div><p></p>
<p>The result above basically looks like a mild rotation and slight warping of the previous result (reproduced below)</p>
<p>
</p><div class="plotly-container">
        <iframe src="https://cdn.math.computer/v/kosmos2/go/dataXdata_380_v0.html" loading="lazy" title="Plotly visualization"></iframe>
        <figcaption class="text-center text-lg text-gray-600 dark:text-gray-400">Seed 21</figcaption>
      </div><p></p>
<p>
</p><div class="flex justify-center">
        <a href="https://cdn.math.computer/v/kosmos2/go/dataXdata_950_v0.html?test" class="inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors">
          Full screen
        </a>
      </div><p></p>
<hr/>
<p>By contrast, the significantly larger dataset of human-captioned images led to a <em>much</em> more stable embedding space result, with only minor variations in structure.</p>
<p>
</p><div class="plotly-container">
        <iframe src="https://cdn.math.computer/2025/1/9/dataXtrain_380.html" loading="lazy" title="Plotly visualization"></iframe>
        <figcaption class="text-center text-lg text-gray-600 dark:text-gray-400">Reduction done on human-labeled captions; seed 42</figcaption>
      </div><p></p>
<p>
</p><div class="flex justify-center">
        <a href="https://cdn.math.computer/2025/1/9/dataXtrain_950.html?test" class="inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors">
          Full screen
        </a>
      </div><p></p>
<p>
</p><div class="plotly-container">
        <iframe src="https://cdn.math.computer/v/kosmos2/go/dataXtrain_380_v0.html" loading="lazy" title="Plotly visualization"></iframe>
        <figcaption class="text-center text-lg text-gray-600 dark:text-gray-400">Seed 21</figcaption>
      </div><p></p>
<p>
</p><div class="flex justify-center">
        <a href="https://cdn.math.computer/v/kosmos2/go/dataXtrain_950_v0.html?test" class="inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors">
          Full screen
        </a>
      </div><p></p>
<p>This is remarkably good behavior being exhibited by <code class="bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm">PaCMAP</code>, which I have not seen to be the case with <code class="bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm">UMAP</code> which in my prior experience has a lot more variation in the results on large datasets.</p>
<p>More reason to keep going with the <code class="bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm">fit</code> on the large dataset, and <code class="bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm">transform</code> smaller ones, when possible.</p>
<p>Both results are aesthetically interesting, but I’ve learned that sensitivity in the final embedding is largely a function of sample size.</p>
<h2>Upgrading embedding models</h2>
<p>Going back to seed 21, I re-ran the pipeline with only the embedding model changing (an unexpected experiment).</p>
<p>
</p><div class="plotly-container">
        <iframe src="https://cdn.math.computer/2025/1/9/dataXdata_380_v0.html" loading="lazy" title="Plotly visualization"></iframe>
        <figcaption class="text-center text-lg text-gray-600 dark:text-gray-400">Reduction done on video frames; seed 21</figcaption>
      </div><p></p>
<p>
</p><div class="flex justify-center">
        <a href="https://cdn.math.computer/2025/1/9/dataXdata_950_v0.html" class="inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors">
          Full screen
        </a>
      </div><p></p>
<p>The embedding where only the frames were used is not too different, but does exhibit slightly different structure (if not similar aeshethics).</p>
<p>However, when I perform a large-scale training first in order to do the embedding, there was a much more dramatic change:</p>
<p>
</p><div class="plotly-container">
        <iframe src="https://cdn.math.computer/2025/1/9/dataXtrain_380_v0.html" loading="lazy" title="Plotly visualization"></iframe>
        <figcaption class="text-center text-lg text-gray-600 dark:text-gray-400">Reduction done on human-labeled captions; seed 21</figcaption>
      </div><p></p>
<p>
</p><div class="flex justify-center">
        <a href="https://cdn.math.computer/2025/1/9/dataXtrain_950_v0.html" class="inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors">
          Full screen
        </a>
      </div><p></p>
<p>I really <em>REALLY</em> like what I’m seeing here now.
There seems to be more logical continuity with respect to the locations of “women” vs “woman” and “groups” being between that and “men” / “man”.
Points are clustered far more closely together, which I am sure will result in a lower distance traveled than anything I’ve done prior.</p>
<p>This says to me that the <code class="bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm">v1.5</code> version of the <code class="bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm">nomic-embed</code> model is a substantial improvement (at least to the eyeball) over <code class="bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm">v1</code>.
There is probably a lot more exploring to do with respect to better performing sentence-embedding models (or just text-embedding in general).</p>
<hr/>
<p>And for good measure, the training dataset itself:</p>
<p>
</p><figure>
        <img alt="" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="w-auto h-auto rounded-md mx-auto min-w-[min(640px,100%)]" style="color:transparent" src="https://cdn.math.computer/2025/1/9/trainXtrain_002.png?test"/>
        <figcaption class="text-center text-sm text-gray-600 dark:text-gray-400">Training data visualized with `nomic-embed-v1.5`</figcaption>
      </figure><p></p>
<p>
</p><div class="flex justify-center">
        <a href="https://cdn.math.computer/2025/1/9/trainXtrain_950_v0.html" class="inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors">
          Full screen
        </a>
      </div><p></p>
<p>Aesthetically (to my mind), this is one of the more pleasing images to have come out of this exercise so far.</p>
<p>I happen to know that there’s a complimentary dataset available from the same source of images that I pulled the captions.
This other dataset is machine-generated labels (much like Kosmos-2 is doing).
I would absolutely love to see the result of comparing human to computer labeled images using this pipeline.
Could I learn to identify machine-generated descriptions of imagery apart from human ones?</p>
<hr/>
<h2>§ coda</h2>
<p>I really loved how my <code class="bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm">Makefile</code> helped prevent careless mistakes with today’s workflow.
As I was switching out seeds and text-embedding models, I could rest assured that the files I was uploading to my content server were not stale.
Human error is easy to encounter; I bounce between writing these posts while waiting for images to generate.
While generating the last image, I forgot that a certain step needed to be run before an image could generate - so it was wonderful that <code class="bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm">make</code> detected the need to re-run the step on my behalf.
The less I need to think about cache-invalidation, the better.</p>
<h2>§§ coda</h2>
<p>I came across a new dimension-reduction algorithm as I was working on some open-source stuff and ran it on the training data.</p>
<p>The result was visually much more compelling than anything else, and exhibits far more separation between clusters of semantically similar captions.</p>
<p>
</p><figure>
        <img alt="" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="w-auto h-auto rounded-md mx-auto min-w-[min(640px,100%)]" style="color:transparent" src="https://cdn.math.computer/2025/1/9/trainXtrain.png?test"/>
        <figcaption class="text-center text-sm text-gray-600 dark:text-gray-400">Training data visualized with `nomic-embed-v1.5` and a new dimension-reduction method</figcaption>
      </figure><p></p>
<p>
</p><div class="flex justify-center">
        <a href="https://cdn.math.computer/2025/1/9/trainXtrain_950.html" class="inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors">
          Full screen
        </a>
      </div><p></p>
<p>The library was in very early stages of development, but now I have motivation to contribute to cleaning it up, as it seems to be doing really well with balancing repulsion and attraction of points.</p>
</div><div class="grid grid-cols-1 md:grid-cols-2 gap-4"></div></article></div></main><nav class="flex justify-between w-full mt-8 mb-4 text-3xl font-bold text-gray-600 dark:text-gray-400 px-4"><div class="flex-1 flex justify-start"><a class="hover:underline" href="/2025/1/8/">←</a></div><div class="flex-1 flex justify-end"><a class="hover:underline" href="/2025/1/10/">→</a></div></nav></div></div><footer class="text-center text-sm text-gray-600 dark:text-gray-400 py-4 bg-white dark:bg-black">© <!-- -->2025<!-- --> Michael Pilosov. All rights reserved.</footer></div><script src="/2025/_next/static/chunks/webpack-b2ce682491b35475.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/2025/_next/static/css/df8c932e539912be.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"2:I[2846,[],\"\"]\n5:I[4707,[],\"\"]\n8:I[6423,[],\"\"]\n9:I[2798,[\"185\",\"static/chunks/app/layout-c0991f7d76273a13.js\"],\"ThemeProvider\"]\na:I[9734,[\"185\",\"static/chunks/app/layout-c0991f7d76273a13.js\"],\"DynamicFavicon\"]\nb:I[8291,[\"185\",\"static/chunks/app/layout-c0991f7d76273a13.js\"],\"Analytics\"]\nd:I[1060,[],\"\"]\n6:[\"month\",\"1\",\"d\"]\n7:[\"day\",\"9\",\"d\"]\ne:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L2\",null,{\"buildId\":\"VQoYmXv-G56ZgJRRzJJFi\",\"assetPrefix\":\"/2025\",\"urlParts\":[\"\",\"1\",\"9\",\"\"],\"initialTree\":[\"\",{\"children\":[[\"month\",\"1\",\"d\"],{\"children\":[[\"day\",\"9\",\"d\"],{\"children\":[\"__PAGE__?{\\\"month\\\":\\\"1\\\",\\\"day\\\":\\\"9\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[[\"month\",\"1\",\"d\"],{\"children\":[[\"day\",\"9\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$L3\",\"$L4\",null],null],null]},[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"$6\",\"children\",\"$7\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"$6\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/2025/_next/static/css/df8c932e539912be.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"className\":\"h-full\",\"children\":[\"$\",\"body\",null,{\"className\":\"min-h-full bg-white dark:bg-black\",\"children\":[[\"$\",\"$L9\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col min-h-full\",\"children\":[[\"$\",\"$La\",null,{}],[\"$\",\"div\",null,{\"className\":\"flex-1 w-full\",\"children\":[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[]}]}],[\"$\",\"footer\",null,{\"className\":\"text-center text-sm text-gray-600 dark:text-gray-400 py-4 bg-white dark:bg-black\",\"children\":[\"© \",2025,\" Michael Pilosov. All rights reserved.\"]}]]}]}],[\"$\",\"$Lb\",null,{}]]}]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Lc\"],\"globalErrorComponent\":\"$d\",\"missingSlots\":\"$We\"}]\n"])</script><script>self.__next_f.push([1,"f:I[2972,[\"450\",\"static/chunks/450-fc8431448c3438bc.js\",\"878\",\"static/chunks/878-5d3156bd3161bc5d.js\",\"160\",\"static/chunks/app/%5Bmonth%5D/%5Bday%5D/page-475f15d3cd396c50.js\"],\"\"]\n10:I[1190,[\"450\",\"static/chunks/450-fc8431448c3438bc.js\",\"878\",\"static/chunks/878-5d3156bd3161bc5d.js\",\"160\",\"static/chunks/app/%5Bmonth%5D/%5Bday%5D/page-475f15d3cd396c50.js\"],\"ThemeToggle\"]\n11:I[5878,[\"450\",\"static/chunks/450-fc8431448c3438bc.js\",\"878\",\"static/chunks/878-5d3156bd3161bc5d.js\",\"160\",\"static/chunks/app/%5Bmonth%5D/%5Bday%5D/page-475f15d3cd396c50.js\"],\"Image\"]\n"])</script><script>self.__next_f.push([1,"4:[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-white dark:bg-black text-black dark:text-white\",\"children\":[[\"$\",\"header\",null,{\"className\":\"flex justify-between items-center mb-8 px-4\",\"children\":[[\"$\",\"$Lf\",null,{\"href\":\"/\",\"className\":\"text-2xl font-bold hover:underline\",\"children\":\"2025 (12 Events)\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center\",\"children\":[[\"$\",\"$Lf\",null,{\"href\":\"/about\",\"className\":\"hover:underline mr-4\",\"children\":\"About\"}],[\"$\",\"$L10\",null,{}]]}]]}],[\"$\",\"nav\",null,{\"className\":\"flex justify-between w-full mb-8 text-3xl font-bold text-gray-600 dark:text-gray-400 px-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex-1 flex justify-start\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"/1/8\",\"className\":\"hover:underline\",\"children\":\"←\"}]}],[\"$\",\"div\",null,{\"className\":\"flex-1 flex justify-end\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"/1/10\",\"className\":\"hover:underline\",\"children\":\"→\"}]}]]}],[\"$\",\"main\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-content mx-auto px-4\",\"children\":[\"$\",\"article\",null,{\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-3xl font-bold mb-2\",\"children\":\"Explorations in Embedding Sensitivity to Initialization\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-600 dark:text-gray-400 mb-4\",\"children\":\"2025-01-09\"}],[\"$\",\"div\",null,{\"className\":\"prose dark:prose-invert mb-8 [\u0026\u003ep]:mb-4 [\u0026_a]:underline [\u0026_a:hover]:text-gray-600 dark:[\u0026_a:hover]:text-gray-300 [\u0026\u003eul]:list-disc [\u0026\u003eul]:pl-6 [\u0026\u003eul]:mb-4 [\u0026\u003eul\u003eli]:pl-2\",\"children\":[[\"$\",\"p\",\"0\",{\"children\":[\"When reviewing my embedding scripts in preparing new images, I found that the \",[\"$\",\"code\",\"1\",{\"className\":\"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm\",\"children\":\"nomic-embed-text-v1.5\"}],\" model card contained incorrect code snippets, which resulted in me using the \",[\"$\",\"code\",\"3\",{\"className\":\"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm\",\"children\":\"v1\"}],\" version of the model.\\nThe results shown prior (and below) are made using the earlier model.\"]}],\"\\n\",[\"$\",\"p\",\"2\",{\"children\":\"Below, I am only changing the seed value for the dimension-reduction model, from 21 to 42 (original, I know).\\nI also continue using the Gondry “GO” video frame results, mostly because they’re still on-disk and I want an apples-to-apples comparison in short time.\"}],\"\\n\",[\"$\",\"hr\",\"4\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"6\",{\"children\":\"\\n\"}],[\"$\",\"div\",\"7\",{\"className\":\"plotly-container\",\"children\":[\"\\n        \",[\"$\",\"iframe\",\"1\",{\"src\":\"https://cdn.math.computer/2025/1/9/dataXdata_380.html\",\"loading\":\"lazy\",\"title\":\"Plotly visualization\",\"children\":\"$undefined\"}],\"\\n        \",[\"$\",\"figcaption\",\"3\",{\"className\":\"text-center text-lg text-gray-600 dark:text-gray-400\",\"children\":\"Reduction done on video frames; seed 42\"}],\"\\n      \"]}],[\"$\",\"p\",\"8\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"10\",{\"children\":\"\\n\"}],[\"$\",\"div\",\"11\",{\"className\":\"flex justify-center\",\"children\":[\"\\n        \",[\"$\",\"a\",\"1\",{\"href\":\"https://cdn.math.computer/2025/1/9/dataXdata_950.html?test\",\"className\":\"inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors\",\"children\":\"\\n          Full screen\\n        \"}],\"\\n      \"]}],[\"$\",\"p\",\"12\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"14\",{\"children\":\"The result above basically looks like a mild rotation and slight warping of the previous result (reproduced below)\"}],\"\\n\",[\"$\",\"p\",\"16\",{\"children\":\"\\n\"}],[\"$\",\"div\",\"17\",{\"className\":\"plotly-container\",\"children\":[\"\\n        \",[\"$\",\"iframe\",\"1\",{\"src\":\"https://cdn.math.computer/v/kosmos2/go/dataXdata_380_v0.html\",\"loading\":\"lazy\",\"title\":\"Plotly visualization\",\"children\":\"$undefined\"}],\"\\n        \",[\"$\",\"figcaption\",\"3\",{\"className\":\"text-center text-lg text-gray-600 dark:text-gray-400\",\"children\":\"Seed 21\"}],\"\\n      \"]}],[\"$\",\"p\",\"18\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"20\",{\"children\":\"\\n\"}],[\"$\",\"div\",\"21\",{\"className\":\"flex justify-center\",\"children\":[\"\\n        \",[\"$\",\"a\",\"1\",{\"href\":\"https://cdn.math.computer/v/kosmos2/go/dataXdata_950_v0.html?test\",\"className\":\"inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors\",\"children\":\"\\n          Full screen\\n        \"}],\"\\n      \"]}],[\"$\",\"p\",\"22\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"hr\",\"24\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"26\",{\"children\":[\"By contrast, the significantly larger dataset of human-captioned images led to a \",[\"$\",\"em\",\"1\",{\"children\":\"much\"}],\" more stable embedding space result, with only minor variations in structure.\"]}],\"\\n\",[\"$\",\"p\",\"28\",{\"children\":\"\\n\"}],[\"$\",\"div\",\"29\",{\"className\":\"plotly-container\",\"children\":[\"\\n        \",[\"$\",\"iframe\",\"1\",{\"src\":\"https://cdn.math.computer/2025/1/9/dataXtrain_380.html\",\"loading\":\"lazy\",\"title\":\"Plotly visualization\",\"children\":\"$undefined\"}],\"\\n        \",[\"$\",\"figcaption\",\"3\",{\"className\":\"text-center text-lg text-gray-600 dark:text-gray-400\",\"children\":\"Reduction done on human-labeled captions; seed 42\"}],\"\\n      \"]}],[\"$\",\"p\",\"30\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"32\",{\"children\":\"\\n\"}],[\"$\",\"div\",\"33\",{\"className\":\"flex justify-center\",\"children\":[\"\\n        \",[\"$\",\"a\",\"1\",{\"href\":\"https://cdn.math.computer/2025/1/9/dataXtrain_950.html?test\",\"className\":\"inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors\",\"children\":\"\\n          Full screen\\n        \"}],\"\\n      \"]}],[\"$\",\"p\",\"34\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"36\",{\"children\":\"\\n\"}],[\"$\",\"div\",\"37\",{\"className\":\"plotly-container\",\"children\":[\"\\n        \",[\"$\",\"iframe\",\"1\",{\"src\":\"https://cdn.math.computer/v/kosmos2/go/dataXtrain_380_v0.html\",\"loading\":\"lazy\",\"title\":\"Plotly visualization\",\"children\":\"$undefined\"}],\"\\n        \",[\"$\",\"figcaption\",\"3\",{\"className\":\"text-center text-lg text-gray-600 dark:text-gray-400\",\"children\":\"Seed 21\"}],\"\\n      \"]}],[\"$\",\"p\",\"38\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"40\",{\"children\":\"\\n\"}],[\"$\",\"div\",\"41\",{\"className\":\"flex justify-center\",\"children\":[\"\\n        \",[\"$\",\"a\",\"1\",{\"href\":\"https://cdn.math.computer/v/kosmos2/go/dataXtrain_950_v0.html?test\",\"className\":\"inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors\",\"children\":\"\\n          Full screen\\n        \"}],\"\\n      \"]}],[\"$\",\"p\",\"42\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"44\",{\"children\":[\"This is remarkably good behavior being exhibited by \",[\"$\",\"code\",\"1\",{\"className\":\"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm\",\"children\":\"PaCMAP\"}],\", which I have not seen to be the case with \",[\"$\",\"code\",\"3\",{\"className\":\"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm\",\"children\":\"UMAP\"}],\" which in my prior experience has a lot more variation in the results on large datasets.\"]}],\"\\n\",[\"$\",\"p\",\"46\",{\"children\":[\"More reason to keep going with the \",[\"$\",\"code\",\"1\",{\"className\":\"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm\",\"children\":\"fit\"}],\" on the large dataset, and \",[\"$\",\"code\",\"3\",{\"className\":\"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm\",\"children\":\"transform\"}],\" smaller ones, when possible.\"]}],\"\\n\",[\"$\",\"p\",\"48\",{\"children\":\"Both results are aesthetically interesting, but I’ve learned that sensitivity in the final embedding is largely a function of sample size.\"}],\"\\n\",[\"$\",\"h2\",\"50\",{\"children\":\"Upgrading embedding models\"}],\"\\n\",[\"$\",\"p\",\"52\",{\"children\":\"Going back to seed 21, I re-ran the pipeline with only the embedding model changing (an unexpected experiment).\"}],\"\\n\",[\"$\",\"p\",\"54\",{\"children\":\"\\n\"}],[\"$\",\"div\",\"55\",{\"className\":\"plotly-container\",\"children\":[\"\\n        \",[\"$\",\"iframe\",\"1\",{\"src\":\"https://cdn.math.computer/2025/1/9/dataXdata_380_v0.html\",\"loading\":\"lazy\",\"title\":\"Plotly visualization\",\"children\":\"$undefined\"}],\"\\n        \",[\"$\",\"figcaption\",\"3\",{\"className\":\"text-center text-lg text-gray-600 dark:text-gray-400\",\"children\":\"Reduction done on video frames; seed 21\"}],\"\\n      \"]}],[\"$\",\"p\",\"56\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"58\",{\"children\":\"\\n\"}],[\"$\",\"div\",\"59\",{\"className\":\"flex justify-center\",\"children\":[\"\\n        \",[\"$\",\"a\",\"1\",{\"href\":\"https://cdn.math.computer/2025/1/9/dataXdata_950_v0.html\",\"className\":\"inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors\",\"children\":\"\\n          Full screen\\n        \"}],\"\\n      \"]}],[\"$\",\"p\",\"60\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"62\",{\"children\":\"The embedding where only the frames were used is not too different, but does exhibit slightly different structure (if not similar aeshethics).\"}],\"\\n\",[\"$\",\"p\",\"64\",{\"children\":\"However, when I perform a large-scale training first in order to do the embedding, there was a much more dramatic change:\"}],\"\\n\",[\"$\",\"p\",\"66\",{\"children\":\"\\n\"}],[\"$\",\"div\",\"67\",{\"className\":\"plotly-container\",\"children\":[\"\\n        \",[\"$\",\"iframe\",\"1\",{\"src\":\"https://cdn.math.computer/2025/1/9/dataXtrain_380_v0.html\",\"loading\":\"lazy\",\"title\":\"Plotly visualization\",\"children\":\"$undefined\"}],\"\\n        \",[\"$\",\"figcaption\",\"3\",{\"className\":\"text-center text-lg text-gray-600 dark:text-gray-400\",\"children\":\"Reduction done on human-labeled captions; seed 21\"}],\"\\n      \"]}],[\"$\",\"p\",\"68\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"70\",{\"children\":\"\\n\"}],[\"$\",\"div\",\"71\",{\"className\":\"flex justify-center\",\"children\":[\"\\n        \",[\"$\",\"a\",\"1\",{\"href\":\"https://cdn.math.computer/2025/1/9/dataXtrain_950_v0.html\",\"className\":\"inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors\",\"children\":\"\\n          Full screen\\n        \"}],\"\\n      \"]}],[\"$\",\"p\",\"72\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"74\",{\"children\":[\"I really \",[\"$\",\"em\",\"1\",{\"children\":\"REALLY\"}],\" like what I’m seeing here now.\\nThere seems to be more logical continuity with respect to the locations of “women” vs “woman” and “groups” being between that and “men” / “man”.\\nPoints are clustered far more closely together, which I am sure will result in a lower distance traveled than anything I’ve done prior.\"]}],\"\\n\",[\"$\",\"p\",\"76\",{\"children\":[\"This says to me that the \",[\"$\",\"code\",\"1\",{\"className\":\"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm\",\"children\":\"v1.5\"}],\" version of the \",[\"$\",\"code\",\"3\",{\"className\":\"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm\",\"children\":\"nomic-embed\"}],\" model is a substantial improvement (at least to the eyeball) over \",[\"$\",\"code\",\"5\",{\"className\":\"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm\",\"children\":\"v1\"}],\".\\nThere is probably a lot more exploring to do with respect to better performing sentence-embedding models (or just text-embedding in general).\"]}],\"\\n\",[\"$\",\"hr\",\"78\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"80\",{\"children\":\"And for good measure, the training dataset itself:\"}],\"\\n\",[\"$\",\"p\",\"82\",{\"children\":\"\\n\"}],[\"$\",\"figure\",\"83\",{\"children\":[\"\\n        \",[\"$\",\"$L11\",\"1\",{\"src\":\"https://cdn.math.computer/2025/1/9/trainXtrain_002.png?test\",\"alt\":\"\",\"width\":800,\"height\":600,\"className\":\"w-auto h-auto rounded-md mx-auto min-w-[min(640px,100%)]\",\"sizes\":\"800px\",\"quality\":78}],\"\\n        \",[\"$\",\"figcaption\",\"3\",{\"className\":\"text-center text-sm text-gray-600 dark:text-gray-400\",\"children\":\"Training data visualized with `nomic-embed-v1.5`\"}],\"\\n      \"]}],[\"$\",\"p\",\"84\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"86\",{\"children\":\"\\n\"}],[\"$\",\"div\",\"87\",{\"className\":\"flex justify-center\",\"children\":[\"\\n        \",[\"$\",\"a\",\"1\",{\"href\":\"https://cdn.math.computer/2025/1/9/trainXtrain_950_v0.html\",\"className\":\"inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors\",\"children\":\"\\n          Full screen\\n        \"}],\"\\n      \"]}],[\"$\",\"p\",\"88\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"90\",{\"children\":\"Aesthetically (to my mind), this is one of the more pleasing images to have come out of this exercise so far.\"}],\"\\n\",[\"$\",\"p\",\"92\",{\"children\":\"I happen to know that there’s a complimentary dataset available from the same source of images that I pulled the captions.\\nThis other dataset is machine-generated labels (much like Kosmos-2 is doing).\\nI would absolutely love to see the result of comparing human to computer labeled images using this pipeline.\\nCould I learn to identify machine-generated descriptions of imagery apart from human ones?\"}],\"\\n\",[\"$\",\"hr\",\"94\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"h2\",\"96\",{\"children\":\"§ coda\"}],\"\\n\",[\"$\",\"p\",\"98\",{\"children\":[\"I really loved how my \",[\"$\",\"code\",\"1\",{\"className\":\"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm\",\"children\":\"Makefile\"}],\" helped prevent careless mistakes with today’s workflow.\\nAs I was switching out seeds and text-embedding models, I could rest assured that the files I was uploading to my content server were not stale.\\nHuman error is easy to encounter; I bounce between writing these posts while waiting for images to generate.\\nWhile generating the last image, I forgot that a certain step needed to be run before an image could generate - so it was wonderful that \",[\"$\",\"code\",\"3\",{\"className\":\"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm\",\"children\":\"make\"}],\" detected the need to re-run the step on my behalf.\\nThe less I need to think about cache-invalidation, the better.\"]}],\"\\n\",[\"$\",\"h2\",\"100\",{\"children\":\"§§ coda\"}],\"\\n\",[\"$\",\"p\",\"102\",{\"children\":\"I came across a new dimension-reduction algorithm as I was working on some open-source stuff and ran it on the training data.\"}],\"\\n\",[\"$\",\"p\",\"104\",{\"children\":\"The result was visually much more compelling than anything else, and exhibits far more separation between clusters of semantically similar captions.\"}],\"\\n\",[\"$\",\"p\",\"106\",{\"children\":\"\\n\"}],[\"$\",\"figure\",\"107\",{\"children\":[\"\\n        \",[\"$\",\"$L11\",\"1\",{\"src\":\"https://cdn.math.computer/2025/1/9/trainXtrain.png?test\",\"alt\":\"\",\"width\":800,\"height\":600,\"className\":\"w-auto h-auto rounded-md mx-auto min-w-[min(640px,100%)]\",\"sizes\":\"800px\",\"quality\":78}],\"\\n        \",[\"$\",\"figcaption\",\"3\",{\"className\":\"text-center text-sm text-gray-600 dark:text-gray-400\",\"children\":\"Training data visualized with `nomic-embed-v1.5` and a new dimension-reduction method\"}],\"\\n      \"]}],[\"$\",\"p\",\"108\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"110\",{\"children\":\"\\n\"}],[\"$\",\"div\",\"111\",{\"className\":\"flex justify-center\",\"children\":[\"\\n        \",[\"$\",\"a\",\"1\",{\"href\":\"https://cdn.math.computer/2025/1/9/trainXtrain_950.html\",\"className\":\"inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors\",\"children\":\"\\n          Full screen\\n        \"}],\"\\n      \"]}],[\"$\",\"p\",\"112\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"114\",{\"children\":\"The library was in very early stages of development, but now I have motivation to contribute to cleaning it up, as it seems to be doing really well with balancing repulsion and attraction of points.\"}],\"\\n\"]}],[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 md:grid-cols-2 gap-4\",\"children\":[]}]]}]}]}],[\"$\",\"nav\",null,{\"className\":\"flex justify-between w-full mt-8 mb-4 text-3xl font-bold text-gray-600 dark:text-gray-400 px-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex-1 flex justify-start\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"/1/8\",\"className\":\"hover:underline\",\"children\":\"←\"}]}],[\"$\",\"div\",null,{\"className\":\"flex-1 flex justify-end\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"/1/10\",\"className\":\"hover:underline\",\"children\":\"→\"}]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"MathematicalMichaelx2025\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Digital Art Log for Dr. Michael Pilosov\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/2025/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"48x48\"}]]\n3:null\n"])</script></body></html>