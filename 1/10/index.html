<!DOCTYPE html><html lang="en" class="h-full"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="https://cdn.math.computer/2025/1/9/trainXtrain_002.png?test" fetchPriority="high"/><link rel="stylesheet" href="/2025/_next/static/css/df8c932e539912be.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/2025/_next/static/chunks/webpack-b2ce682491b35475.js"/><script src="/2025/_next/static/chunks/fd9d1056-a86d52f6b747304e.js" async=""></script><script src="/2025/_next/static/chunks/117-003a6b1a715a95e6.js" async=""></script><script src="/2025/_next/static/chunks/main-app-3c13ae280b9991a1.js" async=""></script><script src="/2025/_next/static/chunks/app/layout-c0991f7d76273a13.js" async=""></script><script src="/2025/_next/static/chunks/450-fc8431448c3438bc.js" async=""></script><script src="/2025/_next/static/chunks/878-5d3156bd3161bc5d.js" async=""></script><script src="/2025/_next/static/chunks/app/%5Bmonth%5D/%5Bday%5D/page-475f15d3cd396c50.js" async=""></script><title>MathematicalMichaelx2025</title><meta name="description" content="Digital Art Log for Dr. Michael Pilosov"/><link rel="icon" href="/2025/favicon.ico" type="image/x-icon" sizes="48x48"/><script src="/2025/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="min-h-full bg-white dark:bg-black"><script>((e,t,r,n,l,o,a,u)=>{let i=document.documentElement,c=["light","dark"];function d(t){(Array.isArray(e)?e:[e]).forEach(e=>{let r="class"===e,n=r&&o?l.map(e=>o[e]||e):l;r?(i.classList.remove(...n),i.classList.add(t)):i.setAttribute(e,t)}),u&&c.includes(t)&&(i.style.colorScheme=t)}if(n)d(n);else try{let e=localStorage.getItem(t)||r,n=a&&"system"===e?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":e;d(n)}catch(e){}})("class","theme","system",null,["light","dark"],null,true,true)</script><div class="flex flex-col min-h-full"><div class="flex-1 w-full"><div class="min-h-screen bg-white dark:bg-black text-black dark:text-white"><header class="flex justify-between items-center mb-8 px-4"><a class="text-2xl font-bold hover:underline" href="/2025/">2025 (12 Events)</a><div class="flex items-center"><a class="hover:underline mr-4" href="/2025/about/">About</a><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 hover:text-accent-foreground h-9 hover:bg-transparent p-0" disabled=""><div class="h-5 w-5"></div><span class="sr-only">Loading theme toggle</span></button></div></header><nav class="flex justify-between w-full mb-8 text-3xl font-bold text-gray-600 dark:text-gray-400 px-4"><div class="flex-1 flex justify-start"><a class="hover:underline" href="/2025/1/9/">←</a></div><div class="flex-1 flex justify-end"><a class="hover:underline" href="/2025/1/11/">→</a></div></nav><main><div class="max-w-content mx-auto px-4"><article><h1 class="text-3xl font-bold mb-2">Quick Notes</h1><p class="text-gray-600 dark:text-gray-400 mb-4">2025-01-10</p><div class="prose dark:prose-invert mb-8 [&amp;&gt;p]:mb-4 [&amp;_a]:underline [&amp;_a:hover]:text-gray-600 dark:[&amp;_a:hover]:text-gray-300 [&amp;&gt;ul]:list-disc [&amp;&gt;ul]:pl-6 [&amp;&gt;ul]:mb-4 [&amp;&gt;ul&gt;li]:pl-2"><h2>Kosmos-2 Updates &amp; Load-Testing</h2>
<p>I made a quick change to my <a href="https://github.com/mindthemath/kosmos-2-api">kosmos-2-api</a> server in order to fix batching, a problem I put aside during my initial tests.
By allowing multiple frames to be sent to the GPU at once, there are efficiencies to be gained (sort of like carrying multiple passengers in a car instead of just the driver).
I found that on my RTX 3090, I had no problem with one API worker and a batch size of 24; the GPU hovered at 50% utilization while processing the frames of the aquarium (my earlier benchmark).
With one worker I got 6-7fps, which was a start.
Even though the GPU would occasionally spike to 75%, it seems like I could run twice the work.
Being impatient, I also doubled the batch size.</p>
<p>
</p><figure>
        <img alt="" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="w-auto h-auto rounded-md mx-auto min-w-[min(640px,100%)]" style="color:transparent" src="https://cdn.math.computer/2025/1/10/gpu_bs48x2.jpg"/>
        <figcaption class="text-center text-sm text-gray-600 dark:text-gray-400">2 workers with batch size 48, ray as client.</figcaption>
      </figure><p></p>
<p>Unfortunately this led to some issues with responses not being returned and retries, so I dialed it back to a batch size of 24 (CUDA ran out of memory).
Fortunately, I liked how well <code class="bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm">litserve</code> handled this (many requests kept getting through).</p>
<p>
</p><figure>
        <img alt="" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="w-auto h-auto rounded-md mx-auto min-w-[min(640px,100%)]" style="color:transparent" src="https://cdn.math.computer/2025/1/10/gpu_bs24x2.jpg"/>
        <figcaption class="text-center text-sm text-gray-600 dark:text-gray-400">2 workers with batch size 24, ray as client.</figcaption>
      </figure><p></p>
<p>This was still too much (as evidenced by memory utilization being near 100% so often - it did occasionally exceed); so I do not have framerates to report.</p>
<p>At a batch size of 48 with a single worker, things went smoothly.</p>
<p>
</p><figure>
        <img alt="" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="w-auto h-auto rounded-md mx-auto min-w-[min(640px,100%)]" style="color:transparent" src="https://cdn.math.computer/2025/1/10/gpu_bs48x1.jpg"/>
        <figcaption class="text-center text-sm text-gray-600 dark:text-gray-400">1 workers with batch size 48, ray as client.</figcaption>
      </figure><p></p>
<p>Unfortunately, this had no practical impact on framerate (I’m guessing at this point the client may be the limit), but I tested a few more configurations.</p>
<p>I went back to five workers with a batch size of 1, which feels like it should be less efficient.
Turns out it was - about half a frame less per second.</p>
<p>So my takeaway so far is to try to find a balance between a few workers and a batch size of 8, 16, or 32 (as the frames do need to come back quickly, afterall… batching too large would increase latency).
To-be-continued on more powerful hardware later…</p>
<hr/>
<h2>Another Embedding Image</h2>
<p>I tried running the path-distance image on the video frames from <a href="/1/9/">yesterday’s post</a> (and a few prior), through the latest promising embedding algorithm, with the dimension-reduction model trained on human-labeled captions.</p>
<p>
</p><figure>
        <img alt="" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="w-auto h-auto rounded-md mx-auto min-w-[min(640px,100%)]" style="color:transparent" src="https://cdn.math.computer/2025/1/10/connect-embed-path-dataXtrain.svg"/>
        <figcaption class="text-center text-sm text-gray-600 dark:text-gray-400">Path of the Gondry GO video with latest embedding</figcaption>
      </figure><p></p>
<p>Interestingly, the path traveled was longer, but overall it feels like many of the same routes were retraces, so there seems to be less variance in the regions visited in total.
This jumpiness is occurring when the model is flipping between identifying the genders of the actors in the video (as well as labeling them as “groups of people” or something similar).
It’s nice that it sort of forms a “star shape” but I am disappointed to see a piece of evidence against my theory that shorter paths correspond to better embedding models.
I can’t say that is definitive, but qualitatively the newer approach seems better when I look at how it handled the training dataset, so I’m more likely to blame my definition of the metric.</p>
<p>Perhaps distance traveled is only a part of the story: step size averages and variance thereof would probably be better indicators.
But for a fixed dimension-reduction model, I do believe that comparing distances traveled from the annotations of two different multi-model language models is a valid thing to do (it’s just that when comparing reduction algorithms that I need to modify my metric).
At the end of the day - the metric that matters most is the aesthetics, though; right?</p>
<p>I may be ready to switch up my dataset for a moment - the annotations from the Kosmos-2 model are a bit too wild, and I’m very curious to see how sentences from more cohesive (human-written) text would be visualized instead.
Some texts I’m very interested in studying the evolution of:</p>
<ul>
<li>Religious Texts - so many ways to break this up - by chapter, book, etc… could be a rich investigation. No copyright issues.</li>
<li>Public-Domain “classics”: what’s war and peace look like?</li>
<li>Screenplays (tricky…)</li>
<li>Email threads / Text Messages (personal…)</li>
<li>My high-school / college essays (cringing at the thought of revisiting them, but the concept of interrogating my past through the lens of modern tools is appealing)</li>
<li>Mathematics textbooks (equation parsing may be challenging - having the LaTeX would help) or class-notes</li>
<li>My doctoral dissertation (this feels like it striks a good balance of length, diversity, and personal subject matter)</li>
</ul>
</div><div class="grid grid-cols-1 md:grid-cols-2 gap-4"><figure class="flex flex-col items-center"><div class="relative overflow-hidden"><img alt="black and white image" fetchPriority="high" loading="eager" width="512" height="512" decoding="async" data-nimg="1" class="w-max-full" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 512 512&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjcwMCIgaGVpZ2h0PSI0NzUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgPGRlZnM+CiAgICA8bGluZWFyR3JhZGllbnQgaWQ9ImciPgogICAgICA8c3RvcCBzdG9wLWNvbG9yPSIjMDAwMCIgb2Zmc2V0PSIyMCUiIC8+CiAgICAgIDxzdG9wIHN0b3AtY29sb3I9IiMwMDAxIiBvZmZzZXQ9IjUwJSIgLz4KICAgICAgPHN0b3Agc3RvcC1jb2xvcj0iIzAwMDAiIG9mZnNldD0iNzAlIiAvPgogICAgPC9saW5lYXJHcmFkaWVudD4KICA8L2RlZnM+CiAgPHJlY3Qgd2lkdGg9IjcwMCIgaGVpZ2h0PSI0NzUiIGZpbGw9IiMwMDAwIiAvPgogIDxyZWN0IGlkPSJyIiB3aWR0aD0iNzAwIiBoZWlnaHQ9IjQ3NSIgZmlsbD0idXJsKCNnKSIgLz4KICA8YW5pbWF0ZSB4bGluazpocmVmPSIjciIgYXR0cmlidXRlTmFtZT0ieCIgZnJvbT0iLTcwMCIgdG89IjcwMCIgZHVyPSIxcyIgcmVwZWF0Q291bnQ9ImluZGVmaW5pdGUiICAvPgo8L3N2Zz4=&#x27;/%3E%3C/svg%3E&quot;)" src="https://cdn.math.computer/2025/1/9/trainXtrain_002.png?test"/></div><figcaption class="mt-2 text-center text-sm text-gray-600 dark:text-gray-400">Previous algorithm of interest</figcaption></figure><figure class="flex flex-col items-center"><div class="relative overflow-hidden"><img alt="black and white image" loading="lazy" width="512" height="512" decoding="async" data-nimg="1" class="w-max-full" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 512 512&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjcwMCIgaGVpZ2h0PSI0NzUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgPGRlZnM+CiAgICA8bGluZWFyR3JhZGllbnQgaWQ9ImciPgogICAgICA8c3RvcCBzdG9wLWNvbG9yPSIjMDAwMCIgb2Zmc2V0PSIyMCUiIC8+CiAgICAgIDxzdG9wIHN0b3AtY29sb3I9IiMwMDAxIiBvZmZzZXQ9IjUwJSIgLz4KICAgICAgPHN0b3Agc3RvcC1jb2xvcj0iIzAwMDAiIG9mZnNldD0iNzAlIiAvPgogICAgPC9saW5lYXJHcmFkaWVudD4KICA8L2RlZnM+CiAgPHJlY3Qgd2lkdGg9IjcwMCIgaGVpZ2h0PSI0NzUiIGZpbGw9IiMwMDAwIiAvPgogIDxyZWN0IGlkPSJyIiB3aWR0aD0iNzAwIiBoZWlnaHQ9IjQ3NSIgZmlsbD0idXJsKCNnKSIgLz4KICA8YW5pbWF0ZSB4bGluazpocmVmPSIjciIgYXR0cmlidXRlTmFtZT0ieCIgZnJvbT0iLTcwMCIgdG89IjcwMCIgZHVyPSIxcyIgcmVwZWF0Q291bnQ9ImluZGVmaW5pdGUiICAvPgo8L3N2Zz4=&#x27;/%3E%3C/svg%3E&quot;)" src="https://cdn.math.computer/2025/1/9/trainXtrain.png?test"/></div><figcaption class="mt-2 text-center text-sm text-gray-600 dark:text-gray-400">New algorithm of interest</figcaption></figure></div></article></div></main><nav class="flex justify-between w-full mt-8 mb-4 text-3xl font-bold text-gray-600 dark:text-gray-400 px-4"><div class="flex-1 flex justify-start"><a class="hover:underline" href="/2025/1/9/">←</a></div><div class="flex-1 flex justify-end"><a class="hover:underline" href="/2025/1/11/">→</a></div></nav></div></div><footer class="text-center text-sm text-gray-600 dark:text-gray-400 py-4 bg-white dark:bg-black">© <!-- -->2025<!-- --> Michael Pilosov. All rights reserved.</footer></div><script src="/2025/_next/static/chunks/webpack-b2ce682491b35475.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/2025/_next/static/css/df8c932e539912be.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"2:I[2846,[],\"\"]\n5:I[4707,[],\"\"]\n8:I[6423,[],\"\"]\n9:I[2798,[\"185\",\"static/chunks/app/layout-c0991f7d76273a13.js\"],\"ThemeProvider\"]\na:I[9734,[\"185\",\"static/chunks/app/layout-c0991f7d76273a13.js\"],\"DynamicFavicon\"]\nb:I[8291,[\"185\",\"static/chunks/app/layout-c0991f7d76273a13.js\"],\"Analytics\"]\nd:I[1060,[],\"\"]\n6:[\"month\",\"1\",\"d\"]\n7:[\"day\",\"10\",\"d\"]\ne:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L2\",null,{\"buildId\":\"VQoYmXv-G56ZgJRRzJJFi\",\"assetPrefix\":\"/2025\",\"urlParts\":[\"\",\"1\",\"10\",\"\"],\"initialTree\":[\"\",{\"children\":[[\"month\",\"1\",\"d\"],{\"children\":[[\"day\",\"10\",\"d\"],{\"children\":[\"__PAGE__?{\\\"month\\\":\\\"1\\\",\\\"day\\\":\\\"10\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[[\"month\",\"1\",\"d\"],{\"children\":[[\"day\",\"10\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$L3\",\"$L4\",null],null],null]},[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"$6\",\"children\",\"$7\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"$6\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/2025/_next/static/css/df8c932e539912be.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"className\":\"h-full\",\"children\":[\"$\",\"body\",null,{\"className\":\"min-h-full bg-white dark:bg-black\",\"children\":[[\"$\",\"$L9\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col min-h-full\",\"children\":[[\"$\",\"$La\",null,{}],[\"$\",\"div\",null,{\"className\":\"flex-1 w-full\",\"children\":[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[]}]}],[\"$\",\"footer\",null,{\"className\":\"text-center text-sm text-gray-600 dark:text-gray-400 py-4 bg-white dark:bg-black\",\"children\":[\"© \",2025,\" Michael Pilosov. All rights reserved.\"]}]]}]}],[\"$\",\"$Lb\",null,{}]]}]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Lc\"],\"globalErrorComponent\":\"$d\",\"missingSlots\":\"$We\"}]\n"])</script><script>self.__next_f.push([1,"f:I[2972,[\"450\",\"static/chunks/450-fc8431448c3438bc.js\",\"878\",\"static/chunks/878-5d3156bd3161bc5d.js\",\"160\",\"static/chunks/app/%5Bmonth%5D/%5Bday%5D/page-475f15d3cd396c50.js\"],\"\"]\n10:I[1190,[\"450\",\"static/chunks/450-fc8431448c3438bc.js\",\"878\",\"static/chunks/878-5d3156bd3161bc5d.js\",\"160\",\"static/chunks/app/%5Bmonth%5D/%5Bday%5D/page-475f15d3cd396c50.js\"],\"ThemeToggle\"]\n11:I[5878,[\"450\",\"static/chunks/450-fc8431448c3438bc.js\",\"878\",\"static/chunks/878-5d3156bd3161bc5d.js\",\"160\",\"static/chunks/app/%5Bmonth%5D/%5Bday%5D/page-475f15d3cd396c50.js\"],\"Image\"]\n"])</script><script>self.__next_f.push([1,"4:[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-white dark:bg-black text-black dark:text-white\",\"children\":[[\"$\",\"header\",null,{\"className\":\"flex justify-between items-center mb-8 px-4\",\"children\":[[\"$\",\"$Lf\",null,{\"href\":\"/\",\"className\":\"text-2xl font-bold hover:underline\",\"children\":\"2025 (12 Events)\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center\",\"children\":[[\"$\",\"$Lf\",null,{\"href\":\"/about\",\"className\":\"hover:underline mr-4\",\"children\":\"About\"}],[\"$\",\"$L10\",null,{}]]}]]}],[\"$\",\"nav\",null,{\"className\":\"flex justify-between w-full mb-8 text-3xl font-bold text-gray-600 dark:text-gray-400 px-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex-1 flex justify-start\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"/1/9\",\"className\":\"hover:underline\",\"children\":\"←\"}]}],[\"$\",\"div\",null,{\"className\":\"flex-1 flex justify-end\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"/1/11\",\"className\":\"hover:underline\",\"children\":\"→\"}]}]]}],[\"$\",\"main\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-content mx-auto px-4\",\"children\":[\"$\",\"article\",null,{\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-3xl font-bold mb-2\",\"children\":\"Quick Notes\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-600 dark:text-gray-400 mb-4\",\"children\":\"2025-01-10\"}],[\"$\",\"div\",null,{\"className\":\"prose dark:prose-invert mb-8 [\u0026\u003ep]:mb-4 [\u0026_a]:underline [\u0026_a:hover]:text-gray-600 dark:[\u0026_a:hover]:text-gray-300 [\u0026\u003eul]:list-disc [\u0026\u003eul]:pl-6 [\u0026\u003eul]:mb-4 [\u0026\u003eul\u003eli]:pl-2\",\"children\":[[\"$\",\"h2\",\"0\",{\"children\":\"Kosmos-2 Updates \u0026 Load-Testing\"}],\"\\n\",[\"$\",\"p\",\"2\",{\"children\":[\"I made a quick change to my \",[\"$\",\"a\",\"1\",{\"href\":\"https://github.com/mindthemath/kosmos-2-api\",\"children\":\"kosmos-2-api\"}],\" server in order to fix batching, a problem I put aside during my initial tests.\\nBy allowing multiple frames to be sent to the GPU at once, there are efficiencies to be gained (sort of like carrying multiple passengers in a car instead of just the driver).\\nI found that on my RTX 3090, I had no problem with one API worker and a batch size of 24; the GPU hovered at 50% utilization while processing the frames of the aquarium (my earlier benchmark).\\nWith one worker I got 6-7fps, which was a start.\\nEven though the GPU would occasionally spike to 75%, it seems like I could run twice the work.\\nBeing impatient, I also doubled the batch size.\"]}],\"\\n\",[\"$\",\"p\",\"4\",{\"children\":\"\\n\"}],[\"$\",\"figure\",\"5\",{\"children\":[\"\\n        \",[\"$\",\"$L11\",\"1\",{\"src\":\"https://cdn.math.computer/2025/1/10/gpu_bs48x2.jpg\",\"alt\":\"\",\"width\":800,\"height\":600,\"className\":\"w-auto h-auto rounded-md mx-auto min-w-[min(640px,100%)]\",\"sizes\":\"800px\",\"quality\":78}],\"\\n        \",[\"$\",\"figcaption\",\"3\",{\"className\":\"text-center text-sm text-gray-600 dark:text-gray-400\",\"children\":\"2 workers with batch size 48, ray as client.\"}],\"\\n      \"]}],[\"$\",\"p\",\"6\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"8\",{\"children\":[\"Unfortunately this led to some issues with responses not being returned and retries, so I dialed it back to a batch size of 24 (CUDA ran out of memory).\\nFortunately, I liked how well \",[\"$\",\"code\",\"1\",{\"className\":\"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm\",\"children\":\"litserve\"}],\" handled this (many requests kept getting through).\"]}],\"\\n\",[\"$\",\"p\",\"10\",{\"children\":\"\\n\"}],[\"$\",\"figure\",\"11\",{\"children\":[\"\\n        \",[\"$\",\"$L11\",\"1\",{\"src\":\"https://cdn.math.computer/2025/1/10/gpu_bs24x2.jpg\",\"alt\":\"\",\"width\":800,\"height\":600,\"className\":\"w-auto h-auto rounded-md mx-auto min-w-[min(640px,100%)]\",\"sizes\":\"800px\",\"quality\":78}],\"\\n        \",[\"$\",\"figcaption\",\"3\",{\"className\":\"text-center text-sm text-gray-600 dark:text-gray-400\",\"children\":\"2 workers with batch size 24, ray as client.\"}],\"\\n      \"]}],[\"$\",\"p\",\"12\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"14\",{\"children\":\"This was still too much (as evidenced by memory utilization being near 100% so often - it did occasionally exceed); so I do not have framerates to report.\"}],\"\\n\",[\"$\",\"p\",\"16\",{\"children\":\"At a batch size of 48 with a single worker, things went smoothly.\"}],\"\\n\",[\"$\",\"p\",\"18\",{\"children\":\"\\n\"}],[\"$\",\"figure\",\"19\",{\"children\":[\"\\n        \",[\"$\",\"$L11\",\"1\",{\"src\":\"https://cdn.math.computer/2025/1/10/gpu_bs48x1.jpg\",\"alt\":\"\",\"width\":800,\"height\":600,\"className\":\"w-auto h-auto rounded-md mx-auto min-w-[min(640px,100%)]\",\"sizes\":\"800px\",\"quality\":78}],\"\\n        \",[\"$\",\"figcaption\",\"3\",{\"className\":\"text-center text-sm text-gray-600 dark:text-gray-400\",\"children\":\"1 workers with batch size 48, ray as client.\"}],\"\\n      \"]}],[\"$\",\"p\",\"20\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"22\",{\"children\":\"Unfortunately, this had no practical impact on framerate (I’m guessing at this point the client may be the limit), but I tested a few more configurations.\"}],\"\\n\",[\"$\",\"p\",\"24\",{\"children\":\"I went back to five workers with a batch size of 1, which feels like it should be less efficient.\\nTurns out it was - about half a frame less per second.\"}],\"\\n\",[\"$\",\"p\",\"26\",{\"children\":\"So my takeaway so far is to try to find a balance between a few workers and a batch size of 8, 16, or 32 (as the frames do need to come back quickly, afterall… batching too large would increase latency).\\nTo-be-continued on more powerful hardware later…\"}],\"\\n\",[\"$\",\"hr\",\"28\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"h2\",\"30\",{\"children\":\"Another Embedding Image\"}],\"\\n\",[\"$\",\"p\",\"32\",{\"children\":[\"I tried running the path-distance image on the video frames from \",[\"$\",\"a\",\"1\",{\"href\":\"/1/9/\",\"children\":\"yesterday’s post\"}],\" (and a few prior), through the latest promising embedding algorithm, with the dimension-reduction model trained on human-labeled captions.\"]}],\"\\n\",[\"$\",\"p\",\"34\",{\"children\":\"\\n\"}],[\"$\",\"figure\",\"35\",{\"children\":[\"\\n        \",[\"$\",\"$L11\",\"1\",{\"src\":\"https://cdn.math.computer/2025/1/10/connect-embed-path-dataXtrain.svg\",\"alt\":\"\",\"width\":800,\"height\":600,\"className\":\"w-auto h-auto rounded-md mx-auto min-w-[min(640px,100%)]\",\"sizes\":\"800px\",\"quality\":78}],\"\\n        \",[\"$\",\"figcaption\",\"3\",{\"className\":\"text-center text-sm text-gray-600 dark:text-gray-400\",\"children\":\"Path of the Gondry GO video with latest embedding\"}],\"\\n      \"]}],[\"$\",\"p\",\"36\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"38\",{\"children\":\"Interestingly, the path traveled was longer, but overall it feels like many of the same routes were retraces, so there seems to be less variance in the regions visited in total.\\nThis jumpiness is occurring when the model is flipping between identifying the genders of the actors in the video (as well as labeling them as “groups of people” or something similar).\\nIt’s nice that it sort of forms a “star shape” but I am disappointed to see a piece of evidence against my theory that shorter paths correspond to better embedding models.\\nI can’t say that is definitive, but qualitatively the newer approach seems better when I look at how it handled the training dataset, so I’m more likely to blame my definition of the metric.\"}],\"\\n\",[\"$\",\"p\",\"40\",{\"children\":\"Perhaps distance traveled is only a part of the story: step size averages and variance thereof would probably be better indicators.\\nBut for a fixed dimension-reduction model, I do believe that comparing distances traveled from the annotations of two different multi-model language models is a valid thing to do (it’s just that when comparing reduction algorithms that I need to modify my metric).\\nAt the end of the day - the metric that matters most is the aesthetics, though; right?\"}],\"\\n\",[\"$\",\"p\",\"42\",{\"children\":\"I may be ready to switch up my dataset for a moment - the annotations from the Kosmos-2 model are a bit too wild, and I’m very curious to see how sentences from more cohesive (human-written) text would be visualized instead.\\nSome texts I’m very interested in studying the evolution of:\"}],\"\\n\",[\"$\",\"ul\",\"44\",{\"children\":[\"\\n\",[\"$\",\"li\",\"1\",{\"children\":\"Religious Texts - so many ways to break this up - by chapter, book, etc… could be a rich investigation. No copyright issues.\"}],\"\\n\",[\"$\",\"li\",\"3\",{\"children\":\"Public-Domain “classics”: what’s war and peace look like?\"}],\"\\n\",[\"$\",\"li\",\"5\",{\"children\":\"Screenplays (tricky…)\"}],\"\\n\",[\"$\",\"li\",\"7\",{\"children\":\"Email threads / Text Messages (personal…)\"}],\"\\n\",[\"$\",\"li\",\"9\",{\"children\":\"My high-school / college essays (cringing at the thought of revisiting them, but the concept of interrogating my past through the lens of modern tools is appealing)\"}],\"\\n\",[\"$\",\"li\",\"11\",{\"children\":\"Mathematics textbooks (equation parsing may be challenging - having the LaTeX would help) or class-notes\"}],\"\\n\",[\"$\",\"li\",\"13\",{\"children\":\"My doctoral dissertation (this feels like it striks a good balance of length, diversity, and personal subject matter)\"}],\"\\n\"]}],\"\\n\"]}],[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 md:grid-cols-2 gap-4\",\"children\":[[\"$\",\"figure\",\"0\",{\"className\":\"flex flex-col items-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"relative overflow-hidden\",\"children\":[\"$\",\"$L11\",null,{\"src\":\"https://cdn.math.computer/2025/1/9/trainXtrain_002.png?test\",\"alt\":\"black and white image\",\"width\":512,\"height\":512,\"className\":\"w-max-full\",\"sizes\":\"512px\",\"quality\":78,\"priority\":true,\"loading\":\"eager\",\"placeholder\":\"blur\",\"blurDataURL\":\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjcwMCIgaGVpZ2h0PSI0NzUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgPGRlZnM+CiAgICA8bGluZWFyR3JhZGllbnQgaWQ9ImciPgogICAgICA8c3RvcCBzdG9wLWNvbG9yPSIjMDAwMCIgb2Zmc2V0PSIyMCUiIC8+CiAgICAgIDxzdG9wIHN0b3AtY29sb3I9IiMwMDAxIiBvZmZzZXQ9IjUwJSIgLz4KICAgICAgPHN0b3Agc3RvcC1jb2xvcj0iIzAwMDAiIG9mZnNldD0iNzAlIiAvPgogICAgPC9saW5lYXJHcmFkaWVudD4KICA8L2RlZnM+CiAgPHJlY3Qgd2lkdGg9IjcwMCIgaGVpZ2h0PSI0NzUiIGZpbGw9IiMwMDAwIiAvPgogIDxyZWN0IGlkPSJyIiB3aWR0aD0iNzAwIiBoZWlnaHQ9IjQ3NSIgZmlsbD0idXJsKCNnKSIgLz4KICA8YW5pbWF0ZSB4bGluazpocmVmPSIjciIgYXR0cmlidXRlTmFtZT0ieCIgZnJvbT0iLTcwMCIgdG89IjcwMCIgZHVyPSIxcyIgcmVwZWF0Q291bnQ9ImluZGVmaW5pdGUiICAvPgo8L3N2Zz4=\"}]}],[\"$\",\"figcaption\",null,{\"className\":\"mt-2 text-center text-sm text-gray-600 dark:text-gray-400\",\"dangerouslySetInnerHTML\":{\"__html\":\"Previous algorithm of interest\"}}]]}],[\"$\",\"figure\",\"1\",{\"className\":\"flex flex-col items-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"relative overflow-hidden\",\"children\":[\"$\",\"$L11\",null,{\"src\":\"https://cdn.math.computer/2025/1/9/trainXtrain.png?test\",\"alt\":\"black and white image\",\"width\":512,\"height\":512,\"className\":\"w-max-full\",\"sizes\":\"512px\",\"quality\":78,\"priority\":false,\"loading\":\"lazy\",\"placeholder\":\"blur\",\"blurDataURL\":\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjcwMCIgaGVpZ2h0PSI0NzUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgPGRlZnM+CiAgICA8bGluZWFyR3JhZGllbnQgaWQ9ImciPgogICAgICA8c3RvcCBzdG9wLWNvbG9yPSIjMDAwMCIgb2Zmc2V0PSIyMCUiIC8+CiAgICAgIDxzdG9wIHN0b3AtY29sb3I9IiMwMDAxIiBvZmZzZXQ9IjUwJSIgLz4KICAgICAgPHN0b3Agc3RvcC1jb2xvcj0iIzAwMDAiIG9mZnNldD0iNzAlIiAvPgogICAgPC9saW5lYXJHcmFkaWVudD4KICA8L2RlZnM+CiAgPHJlY3Qgd2lkdGg9IjcwMCIgaGVpZ2h0PSI0NzUiIGZpbGw9IiMwMDAwIiAvPgogIDxyZWN0IGlkPSJyIiB3aWR0aD0iNzAwIiBoZWlnaHQ9IjQ3NSIgZmlsbD0idXJsKCNnKSIgLz4KICA8YW5pbWF0ZSB4bGluazpocmVmPSIjciIgYXR0cmlidXRlTmFtZT0ieCIgZnJvbT0iLTcwMCIgdG89IjcwMCIgZHVyPSIxcyIgcmVwZWF0Q291bnQ9ImluZGVmaW5pdGUiICAvPgo8L3N2Zz4=\"}]}],[\"$\",\"figcaption\",null,{\"className\":\"mt-2 text-center text-sm text-gray-600 dark:text-gray-400\",\"dangerouslySetInnerHTML\":{\"__html\":\"New algorithm of interest\"}}]]}]]}]]}]}]}],[\"$\",\"nav\",null,{\"className\":\"flex justify-between w-full mt-8 mb-4 text-3xl font-bold text-gray-600 dark:text-gray-400 px-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex-1 flex justify-start\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"/1/9\",\"className\":\"hover:underline\",\"children\":\"←\"}]}],[\"$\",\"div\",null,{\"className\":\"flex-1 flex justify-end\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"/1/11\",\"className\":\"hover:underline\",\"children\":\"→\"}]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"MathematicalMichaelx2025\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Digital Art Log for Dr. Michael Pilosov\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/2025/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"48x48\"}]]\n3:null\n"])</script></body></html>