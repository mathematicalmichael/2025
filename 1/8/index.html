<!DOCTYPE html><html lang="en" class="h-full"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/2025/_next/static/css/df8c932e539912be.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/2025/_next/static/chunks/webpack-b2ce682491b35475.js"/><script src="/2025/_next/static/chunks/fd9d1056-a86d52f6b747304e.js" async=""></script><script src="/2025/_next/static/chunks/117-003a6b1a715a95e6.js" async=""></script><script src="/2025/_next/static/chunks/main-app-3c13ae280b9991a1.js" async=""></script><script src="/2025/_next/static/chunks/app/layout-c0991f7d76273a13.js" async=""></script><script src="/2025/_next/static/chunks/450-fc8431448c3438bc.js" async=""></script><script src="/2025/_next/static/chunks/878-5d3156bd3161bc5d.js" async=""></script><script src="/2025/_next/static/chunks/app/%5Bmonth%5D/%5Bday%5D/page-475f15d3cd396c50.js" async=""></script><title>MathematicalMichaelx2025</title><meta name="description" content="Digital Art Log for Dr. Michael Pilosov"/><link rel="icon" href="/2025/favicon.ico" type="image/x-icon" sizes="48x48"/><script src="/2025/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="min-h-full bg-white dark:bg-black"><script>((e,t,r,n,l,o,a,u)=>{let i=document.documentElement,c=["light","dark"];function d(t){(Array.isArray(e)?e:[e]).forEach(e=>{let r="class"===e,n=r&&o?l.map(e=>o[e]||e):l;r?(i.classList.remove(...n),i.classList.add(t)):i.setAttribute(e,t)}),u&&c.includes(t)&&(i.style.colorScheme=t)}if(n)d(n);else try{let e=localStorage.getItem(t)||r,n=a&&"system"===e?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":e;d(n)}catch(e){}})("class","theme","system",null,["light","dark"],null,true,true)</script><div class="flex flex-col min-h-full"><div class="flex-1 w-full"><div class="min-h-screen bg-white dark:bg-black text-black dark:text-white"><header class="flex justify-between items-center mb-8 px-4"><a class="text-2xl font-bold hover:underline" href="/2025/">2025 (12 Events)</a><div class="flex items-center"><a class="hover:underline mr-4" href="/2025/about/">About</a><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 hover:text-accent-foreground h-9 hover:bg-transparent p-0" disabled=""><div class="h-5 w-5"></div><span class="sr-only">Loading theme toggle</span></button></div></header><nav class="flex justify-between w-full mb-8 text-3xl font-bold text-gray-600 dark:text-gray-400 px-4"><div class="flex-1 flex justify-start"><a class="hover:underline" href="/2025/1/7/">←</a></div><div class="flex-1 flex justify-end"><a class="hover:underline" href="/2025/1/9/">→</a></div></nav><main><div class="max-w-content mx-auto px-4"><article><h1 class="text-3xl font-bold mb-2">Mathematics &amp; Aesthetics</h1><p class="text-gray-600 dark:text-gray-400 mb-4">2025-01-08</p><div class="prose dark:prose-invert mb-8 [&amp;&gt;p]:mb-4 [&amp;_a]:underline [&amp;_a:hover]:text-gray-600 dark:[&amp;_a:hover]:text-gray-300 [&amp;&gt;ul]:list-disc [&amp;&gt;ul]:pl-6 [&amp;&gt;ul]:mb-4 [&amp;&gt;ul&gt;li]:pl-2"><h2>Observations from Sunday’s Embeddings</h2>
<p>Revisiting the embeddings of the Michel Gondry music video that I studied on <a href="/1/6/">1/6</a>, particularly the ones that were based on human-labeled annotations, I spotted some more interesting trends in the data.</p>
<p>As a refresher, here are the embeddings I’ll be pulling from:</p>
<p>
</p><div class="plotly-container">
        <iframe src="https://cdn.math.computer/v/kosmos2/go/dataXtrain_380_v0.html?test" loading="lazy" title="Plotly visualization"></iframe>
        <figcaption class="text-center text-lg text-gray-600 dark:text-gray-400">Embedding trained on captions</figcaption>
      </div><p></p>
<p>
</p><div class="flex justify-center">
        <a href="https://cdn.math.computer/v/kosmos2/go/dataXtrain_950_v0.html?test" class="inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors">
          Full screen version
        </a>
      </div><p></p>
<p>I’ve labeled some such observations - clusters that represent “groups” vs “men” and “women” are prety easy to spot.</p>
<p>
</p><figure>
        <img alt="" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="w-auto h-auto rounded-md mx-auto min-w-[min(640px,100%)]" style="color:transparent" src="https://cdn.math.computer/2025/1/8/annotated.jpg"/>
        <figcaption class="text-center text-sm text-gray-600 dark:text-gray-400">Clusters labeled by me</figcaption>
      </figure><p></p>
<p>Most of the “mass” is between the “groups” cluster and the “groups of women” cluster.
Mistakes such as “statues” appear quite far away, near the bottom of the plot.</p>
<h2>Goals</h2>
<p>What I’ve been exploring in this series of posts is to interrogate the understanding of machines through visual means.
I ask myself: “what characteristics would a machine exhibit that understands visual information the way a human does?”</p>
<p>To me one of the most important characteristics of a well-calibrated model is this: there should be very low sensitivity to subtle changes.
A person perceives smooth motion at 24-30 frames per second, but any individual pair of subsequent frames is extremely similar (excepting scene transitions).
One’s understanding of a scene rarely undergoes a dramatic shift from frame to frame - rather it is from changes on scales of seconds (or longer time-horizons).
Therefore, a model that has similar understanding of imagery would also not exhibit radical changes on sub-second timescales.</p>
<h2>Analytical Interpretation</h2>
<p>Before I even began to create an image, I used my understanding of how these systems fit together to set a hypothesis that I would end up testing.</p>
<blockquote>
<p>What I’ve been exploring with this series of posts is to interrogate the understanding of machines through visual means.</p>
</blockquote>
<p>I find it reasonable to posit that a machine that performs well exhibits low frame-to-frame travel distances in embedding space.</p>
<p>There is a subtlety here - because there are two models I am simultaneously relying on here - one to create the frame annotations, and another to create a two-dimensional representation of that textual information.
Technically - there are three models, because in between those two steps is a model that turns text into vectors, which has been trained by other contributors.
However, in the context of my explorations I am only training <em>one</em> model - the one responsible for collapsing hundreds or thousands of dimensions into 2 (which I can easily plot).</p>
<p>To restate the point above - the dimension reduction model I seek is one that has stable interpretability across many different datasets.
To visualize <code class="bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm">Kosmos-2</code>’s performance (which is what I’m interested in), I want to find an approach to plotting its answers as a flat image.
This approach needs to exhibit low sensitivity to subtle changes in text.</p>
<p>Translating this to visual phenomenon - I’ll know I’m on the right track if the images I generate have tight clusters of lines, representing many small steps between frames instead of large jumps.
Since the lines are partially transparent, concentrations of white represent a density of points and a continuity among them from a temporal (frame-to-frame) perspective.</p>
<p>First - here is the path taken by Kosmos-2, as interpreted in isolation (the dimension-reduction model is not exposed to sentences outside of those Kosmos-2 generated).</p>
<p>
</p><figure>
        <img alt="" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="w-auto h-auto rounded-md mx-auto min-w-[min(640px,100%)]" style="color:transparent" src="https://cdn.math.computer/2025/1/8/connect-embed-path-dataXdata.svg"/>
        <figcaption class="text-center text-sm text-gray-600 dark:text-gray-400">Fig. 1a: Fitting on Kosmos-2 annotations</figcaption>
      </figure><p></p>
<p>Fig. 1a represents the “usual workflow” in exploratory data analysis - running <code class="bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm">fit</code> and <code class="bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm">transform</code> steps on the same data.</p>
<p>When I first fit a dimension-reduction model on embeddings from ~500k human captions of imagery and visualize the same set of Kosmos-2 sentences, I get exactly what I expected: a lot more density in the image, shorter paths, and a significantly lower distance traveled:</p>
<p>
</p><figure>
        <img alt="" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="w-auto h-auto rounded-md mx-auto min-w-[min(640px,100%)]" style="color:transparent" src="https://cdn.math.computer/2025/1/8/connect-embed-path-dataXtrain.svg"/>
        <figcaption class="text-center text-sm text-gray-600 dark:text-gray-400">Fig. 2a: Fitting on human labels</figcaption>
      </figure><p></p>
<p>There is much more strong presence of bright white in this image.
The path through semantic-space that the same set of Kosmos-2 annotations took was half the length of the approach that one would take when analyzing data in isolation.</p>
<p>There seems to be a lot of jumping in the horizontal direction in Fig. 2a.
These two areas correspond to the “women cluster” (right) and “group cluster” (left), which tracks with what I noticed in the video (trouble identifying the costumed performers).</p>
<p>
</p><figure>
        <video controls="">
          <source src="https://cdn.math.computer/v/kosmos2/go/output.mp4" type="video/mp4"/>
          Your browser does not support the video tag.
        </video>
        <figcaption class="text-center text-sm text-gray-600 dark:text-gray-400">GO - The Chemical Brothers, 2005. Dir. Michel Gondry</figcaption>
      </figure><p></p>
<p>Taking a look at the histograms of the two respective embeddings, we can see that there’s a lot less variance in the embeddings, and more unique values implies that sentences that are subtly different are very close together in space.
This is further evidenced in the y-distribution (#1 in the image) concentrating around one primary range of values, which tracks with the horizontal lines in the image.</p>
<p>
</p><figure>
        <img alt="" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="w-auto h-auto rounded-md mx-auto min-w-[min(640px,100%)]" style="color:transparent" src="https://cdn.math.computer/2025/1/8/hist_data.jpg"/>
        <figcaption class="text-center text-sm text-gray-600 dark:text-gray-400">Fig. 1b: Histograms of 2D embeddings based on Kosmos-2 annotations</figcaption>
      </figure><p></p>
<p>
</p><figure>
        <img alt="" loading="lazy" width="800" height="600" decoding="async" data-nimg="1" class="w-auto h-auto rounded-md mx-auto min-w-[min(640px,100%)]" style="color:transparent" src="https://cdn.math.computer/2025/1/8/hist_train.jpg"/>
        <figcaption class="text-center text-sm text-gray-600 dark:text-gray-400">Fig. 2b: Histograms of 2D embeddedings based on human labels</figcaption>
      </figure><p></p>
<h2>Takeaways</h2>
<p>Stepping outside of art for a moment, I think this exercise revealed to me that when using dimension reduction models for exploratory data analysis, it really helps to separate the <code class="bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm">fit</code> and <code class="bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm">transform</code> steps out, using different datasets for each.</p>
<p>Using <code class="bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm">fit_transform</code>, in essence, is a fine exercise to get a quick glance at your data.
But when it comes to really understanding what something means, it behooves the practitioner to train a dimension-reduction model on a diverse and representative dataset of interest, and re-use it for inference.</p>
<p>However, if comparing multiple models upstream of the dimension-reduction process (e.g., swapping out <code class="bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm">Kosmos-2</code> or <code class="bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm">nomic-ai/nomic-embed-text-v1</code>), then the <code class="bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm">fit_transform</code> approach would be helpful for defining a metric to measure temporal stability.
Given a fixed seed, and multiple models, the ones that travel a smaller distance are “more stable.”</p>
<p>Instead of temporal stability, one can apply transformations to images that are either imperceptible to humans (e.g., small amounts of random noise), or would not impact their interpetation of the scene (e.g., a color tint).
The same travel-distance metric would be appropriate for comparing those experiments to one another, and would work for both freshly-fit-with-same-seed and pre-trained reduction models.</p>
<p>For me - when it comes to interpreting text annotations of video frames, I’m going to be sticking to using models I train on large quantities of data.
However, combining the two visuals as I did <a href="/1/7/">yesterday</a> is also a viable option that shows an interesting contrast.</p>
<h2>Next Steps</h2>
<p>I’m going to repeat this process of generating images for several seeds, to study the sensitivity of the dimension-reduction model, and explore the visual diversity of using this particular framework.</p>
<p>Then I’ll move on to comparing several dimension-reduction algorithms on synthetic data to really understand (in a controlled setting), what visual phenomenon are exhibited by the different frameworks.</p>
</div><div class="grid grid-cols-1 md:grid-cols-2 gap-4"></div></article></div></main><nav class="flex justify-between w-full mt-8 mb-4 text-3xl font-bold text-gray-600 dark:text-gray-400 px-4"><div class="flex-1 flex justify-start"><a class="hover:underline" href="/2025/1/7/">←</a></div><div class="flex-1 flex justify-end"><a class="hover:underline" href="/2025/1/9/">→</a></div></nav></div></div><footer class="text-center text-sm text-gray-600 dark:text-gray-400 py-4 bg-white dark:bg-black">© <!-- -->2025<!-- --> Michael Pilosov. All rights reserved.</footer></div><script src="/2025/_next/static/chunks/webpack-b2ce682491b35475.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/2025/_next/static/css/df8c932e539912be.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"2:I[2846,[],\"\"]\n5:I[4707,[],\"\"]\n8:I[6423,[],\"\"]\n9:I[2798,[\"185\",\"static/chunks/app/layout-c0991f7d76273a13.js\"],\"ThemeProvider\"]\na:I[9734,[\"185\",\"static/chunks/app/layout-c0991f7d76273a13.js\"],\"DynamicFavicon\"]\nb:I[8291,[\"185\",\"static/chunks/app/layout-c0991f7d76273a13.js\"],\"Analytics\"]\nd:I[1060,[],\"\"]\n6:[\"month\",\"1\",\"d\"]\n7:[\"day\",\"8\",\"d\"]\ne:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L2\",null,{\"buildId\":\"VQoYmXv-G56ZgJRRzJJFi\",\"assetPrefix\":\"/2025\",\"urlParts\":[\"\",\"1\",\"8\",\"\"],\"initialTree\":[\"\",{\"children\":[[\"month\",\"1\",\"d\"],{\"children\":[[\"day\",\"8\",\"d\"],{\"children\":[\"__PAGE__?{\\\"month\\\":\\\"1\\\",\\\"day\\\":\\\"8\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[[\"month\",\"1\",\"d\"],{\"children\":[[\"day\",\"8\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$L3\",\"$L4\",null],null],null]},[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"$6\",\"children\",\"$7\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"$6\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/2025/_next/static/css/df8c932e539912be.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"className\":\"h-full\",\"children\":[\"$\",\"body\",null,{\"className\":\"min-h-full bg-white dark:bg-black\",\"children\":[[\"$\",\"$L9\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col min-h-full\",\"children\":[[\"$\",\"$La\",null,{}],[\"$\",\"div\",null,{\"className\":\"flex-1 w-full\",\"children\":[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[]}]}],[\"$\",\"footer\",null,{\"className\":\"text-center text-sm text-gray-600 dark:text-gray-400 py-4 bg-white dark:bg-black\",\"children\":[\"© \",2025,\" Michael Pilosov. All rights reserved.\"]}]]}]}],[\"$\",\"$Lb\",null,{}]]}]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Lc\"],\"globalErrorComponent\":\"$d\",\"missingSlots\":\"$We\"}]\n"])</script><script>self.__next_f.push([1,"f:I[2972,[\"450\",\"static/chunks/450-fc8431448c3438bc.js\",\"878\",\"static/chunks/878-5d3156bd3161bc5d.js\",\"160\",\"static/chunks/app/%5Bmonth%5D/%5Bday%5D/page-475f15d3cd396c50.js\"],\"\"]\n10:I[1190,[\"450\",\"static/chunks/450-fc8431448c3438bc.js\",\"878\",\"static/chunks/878-5d3156bd3161bc5d.js\",\"160\",\"static/chunks/app/%5Bmonth%5D/%5Bday%5D/page-475f15d3cd396c50.js\"],\"ThemeToggle\"]\n11:I[5878,[\"450\",\"static/chunks/450-fc8431448c3438bc.js\",\"878\",\"static/chunks/878-5d3156bd3161bc5d.js\",\"160\",\"static/chunks/app/%5Bmonth%5D/%5Bday%5D/page-475f15d3cd396c50.js\"],\"Image\"]\n"])</script><script>self.__next_f.push([1,"4:[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-white dark:bg-black text-black dark:text-white\",\"children\":[[\"$\",\"header\",null,{\"className\":\"flex justify-between items-center mb-8 px-4\",\"children\":[[\"$\",\"$Lf\",null,{\"href\":\"/\",\"className\":\"text-2xl font-bold hover:underline\",\"children\":\"2025 (12 Events)\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center\",\"children\":[[\"$\",\"$Lf\",null,{\"href\":\"/about\",\"className\":\"hover:underline mr-4\",\"children\":\"About\"}],[\"$\",\"$L10\",null,{}]]}]]}],[\"$\",\"nav\",null,{\"className\":\"flex justify-between w-full mb-8 text-3xl font-bold text-gray-600 dark:text-gray-400 px-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex-1 flex justify-start\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"/1/7\",\"className\":\"hover:underline\",\"children\":\"←\"}]}],[\"$\",\"div\",null,{\"className\":\"flex-1 flex justify-end\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"/1/9\",\"className\":\"hover:underline\",\"children\":\"→\"}]}]]}],[\"$\",\"main\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-content mx-auto px-4\",\"children\":[\"$\",\"article\",null,{\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-3xl font-bold mb-2\",\"children\":\"Mathematics \u0026 Aesthetics\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-600 dark:text-gray-400 mb-4\",\"children\":\"2025-01-08\"}],[\"$\",\"div\",null,{\"className\":\"prose dark:prose-invert mb-8 [\u0026\u003ep]:mb-4 [\u0026_a]:underline [\u0026_a:hover]:text-gray-600 dark:[\u0026_a:hover]:text-gray-300 [\u0026\u003eul]:list-disc [\u0026\u003eul]:pl-6 [\u0026\u003eul]:mb-4 [\u0026\u003eul\u003eli]:pl-2\",\"children\":[[\"$\",\"h2\",\"0\",{\"children\":\"Observations from Sunday’s Embeddings\"}],\"\\n\",[\"$\",\"p\",\"2\",{\"children\":[\"Revisiting the embeddings of the Michel Gondry music video that I studied on \",[\"$\",\"a\",\"1\",{\"href\":\"/1/6/\",\"children\":\"1/6\"}],\", particularly the ones that were based on human-labeled annotations, I spotted some more interesting trends in the data.\"]}],\"\\n\",[\"$\",\"p\",\"4\",{\"children\":\"As a refresher, here are the embeddings I’ll be pulling from:\"}],\"\\n\",[\"$\",\"p\",\"6\",{\"children\":\"\\n\"}],[\"$\",\"div\",\"7\",{\"className\":\"plotly-container\",\"children\":[\"\\n        \",[\"$\",\"iframe\",\"1\",{\"src\":\"https://cdn.math.computer/v/kosmos2/go/dataXtrain_380_v0.html?test\",\"loading\":\"lazy\",\"title\":\"Plotly visualization\",\"children\":\"$undefined\"}],\"\\n        \",[\"$\",\"figcaption\",\"3\",{\"className\":\"text-center text-lg text-gray-600 dark:text-gray-400\",\"children\":\"Embedding trained on captions\"}],\"\\n      \"]}],[\"$\",\"p\",\"8\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"10\",{\"children\":\"\\n\"}],[\"$\",\"div\",\"11\",{\"className\":\"flex justify-center\",\"children\":[\"\\n        \",[\"$\",\"a\",\"1\",{\"href\":\"https://cdn.math.computer/v/kosmos2/go/dataXtrain_950_v0.html?test\",\"className\":\"inline-block px-4 py-2 bg-neutral-100 dark:bg-neutral-800 hover:bg-neutral-200 dark:hover:bg-neutral-700 rounded-md text-lg transition-colors\",\"children\":\"\\n          Full screen version\\n        \"}],\"\\n      \"]}],[\"$\",\"p\",\"12\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"14\",{\"children\":\"I’ve labeled some such observations - clusters that represent “groups” vs “men” and “women” are prety easy to spot.\"}],\"\\n\",[\"$\",\"p\",\"16\",{\"children\":\"\\n\"}],[\"$\",\"figure\",\"17\",{\"children\":[\"\\n        \",[\"$\",\"$L11\",\"1\",{\"src\":\"https://cdn.math.computer/2025/1/8/annotated.jpg\",\"alt\":\"\",\"width\":800,\"height\":600,\"className\":\"w-auto h-auto rounded-md mx-auto min-w-[min(640px,100%)]\",\"sizes\":\"800px\",\"quality\":78}],\"\\n        \",[\"$\",\"figcaption\",\"3\",{\"className\":\"text-center text-sm text-gray-600 dark:text-gray-400\",\"children\":\"Clusters labeled by me\"}],\"\\n      \"]}],[\"$\",\"p\",\"18\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"20\",{\"children\":\"Most of the “mass” is between the “groups” cluster and the “groups of women” cluster.\\nMistakes such as “statues” appear quite far away, near the bottom of the plot.\"}],\"\\n\",[\"$\",\"h2\",\"22\",{\"children\":\"Goals\"}],\"\\n\",[\"$\",\"p\",\"24\",{\"children\":\"What I’ve been exploring in this series of posts is to interrogate the understanding of machines through visual means.\\nI ask myself: “what characteristics would a machine exhibit that understands visual information the way a human does?”\"}],\"\\n\",[\"$\",\"p\",\"26\",{\"children\":\"To me one of the most important characteristics of a well-calibrated model is this: there should be very low sensitivity to subtle changes.\\nA person perceives smooth motion at 24-30 frames per second, but any individual pair of subsequent frames is extremely similar (excepting scene transitions).\\nOne’s understanding of a scene rarely undergoes a dramatic shift from frame to frame - rather it is from changes on scales of seconds (or longer time-horizons).\\nTherefore, a model that has similar understanding of imagery would also not exhibit radical changes on sub-second timescales.\"}],\"\\n\",[\"$\",\"h2\",\"28\",{\"children\":\"Analytical Interpretation\"}],\"\\n\",[\"$\",\"p\",\"30\",{\"children\":\"Before I even began to create an image, I used my understanding of how these systems fit together to set a hypothesis that I would end up testing.\"}],\"\\n\",[\"$\",\"blockquote\",\"32\",{\"children\":[\"\\n\",[\"$\",\"p\",\"1\",{\"children\":\"What I’ve been exploring with this series of posts is to interrogate the understanding of machines through visual means.\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",\"34\",{\"children\":\"I find it reasonable to posit that a machine that performs well exhibits low frame-to-frame travel distances in embedding space.\"}],\"\\n\",[\"$\",\"p\",\"36\",{\"children\":[\"There is a subtlety here - because there are two models I am simultaneously relying on here - one to create the frame annotations, and another to create a two-dimensional representation of that textual information.\\nTechnically - there are three models, because in between those two steps is a model that turns text into vectors, which has been trained by other contributors.\\nHowever, in the context of my explorations I am only training \",[\"$\",\"em\",\"1\",{\"children\":\"one\"}],\" model - the one responsible for collapsing hundreds or thousands of dimensions into 2 (which I can easily plot).\"]}],\"\\n\",[\"$\",\"p\",\"38\",{\"children\":[\"To restate the point above - the dimension reduction model I seek is one that has stable interpretability across many different datasets.\\nTo visualize \",[\"$\",\"code\",\"1\",{\"className\":\"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm\",\"children\":\"Kosmos-2\"}],\"’s performance (which is what I’m interested in), I want to find an approach to plotting its answers as a flat image.\\nThis approach needs to exhibit low sensitivity to subtle changes in text.\"]}],\"\\n\",[\"$\",\"p\",\"40\",{\"children\":\"Translating this to visual phenomenon - I’ll know I’m on the right track if the images I generate have tight clusters of lines, representing many small steps between frames instead of large jumps.\\nSince the lines are partially transparent, concentrations of white represent a density of points and a continuity among them from a temporal (frame-to-frame) perspective.\"}],\"\\n\",[\"$\",\"p\",\"42\",{\"children\":\"First - here is the path taken by Kosmos-2, as interpreted in isolation (the dimension-reduction model is not exposed to sentences outside of those Kosmos-2 generated).\"}],\"\\n\",[\"$\",\"p\",\"44\",{\"children\":\"\\n\"}],[\"$\",\"figure\",\"45\",{\"children\":[\"\\n        \",[\"$\",\"$L11\",\"1\",{\"src\":\"https://cdn.math.computer/2025/1/8/connect-embed-path-dataXdata.svg\",\"alt\":\"\",\"width\":800,\"height\":600,\"className\":\"w-auto h-auto rounded-md mx-auto min-w-[min(640px,100%)]\",\"sizes\":\"800px\",\"quality\":78}],\"\\n        \",[\"$\",\"figcaption\",\"3\",{\"className\":\"text-center text-sm text-gray-600 dark:text-gray-400\",\"children\":\"Fig. 1a: Fitting on Kosmos-2 annotations\"}],\"\\n      \"]}],[\"$\",\"p\",\"46\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"48\",{\"children\":[\"Fig. 1a represents the “usual workflow” in exploratory data analysis - running \",[\"$\",\"code\",\"1\",{\"className\":\"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm\",\"children\":\"fit\"}],\" and \",[\"$\",\"code\",\"3\",{\"className\":\"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm\",\"children\":\"transform\"}],\" steps on the same data.\"]}],\"\\n\",[\"$\",\"p\",\"50\",{\"children\":\"When I first fit a dimension-reduction model on embeddings from ~500k human captions of imagery and visualize the same set of Kosmos-2 sentences, I get exactly what I expected: a lot more density in the image, shorter paths, and a significantly lower distance traveled:\"}],\"\\n\",[\"$\",\"p\",\"52\",{\"children\":\"\\n\"}],[\"$\",\"figure\",\"53\",{\"children\":[\"\\n        \",[\"$\",\"$L11\",\"1\",{\"src\":\"https://cdn.math.computer/2025/1/8/connect-embed-path-dataXtrain.svg\",\"alt\":\"\",\"width\":800,\"height\":600,\"className\":\"w-auto h-auto rounded-md mx-auto min-w-[min(640px,100%)]\",\"sizes\":\"800px\",\"quality\":78}],\"\\n        \",[\"$\",\"figcaption\",\"3\",{\"className\":\"text-center text-sm text-gray-600 dark:text-gray-400\",\"children\":\"Fig. 2a: Fitting on human labels\"}],\"\\n      \"]}],[\"$\",\"p\",\"54\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"56\",{\"children\":\"There is much more strong presence of bright white in this image.\\nThe path through semantic-space that the same set of Kosmos-2 annotations took was half the length of the approach that one would take when analyzing data in isolation.\"}],\"\\n\",[\"$\",\"p\",\"58\",{\"children\":\"There seems to be a lot of jumping in the horizontal direction in Fig. 2a.\\nThese two areas correspond to the “women cluster” (right) and “group cluster” (left), which tracks with what I noticed in the video (trouble identifying the costumed performers).\"}],\"\\n\",[\"$\",\"p\",\"60\",{\"children\":\"\\n\"}],[\"$\",\"figure\",\"61\",{\"children\":[\"\\n        \",[\"$\",\"video\",\"1\",{\"controls\":true,\"children\":[\"\\n          \",[\"$\",\"source\",\"1\",{\"src\":\"https://cdn.math.computer/v/kosmos2/go/output.mp4\",\"type\":\"video/mp4\",\"children\":\"$undefined\"}],\"\\n          Your browser does not support the video tag.\\n        \"]}],\"\\n        \",[\"$\",\"figcaption\",\"3\",{\"className\":\"text-center text-sm text-gray-600 dark:text-gray-400\",\"children\":\"GO - The Chemical Brothers, 2005. Dir. Michel Gondry\"}],\"\\n      \"]}],[\"$\",\"p\",\"62\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"64\",{\"children\":\"Taking a look at the histograms of the two respective embeddings, we can see that there’s a lot less variance in the embeddings, and more unique values implies that sentences that are subtly different are very close together in space.\\nThis is further evidenced in the y-distribution (#1 in the image) concentrating around one primary range of values, which tracks with the horizontal lines in the image.\"}],\"\\n\",[\"$\",\"p\",\"66\",{\"children\":\"\\n\"}],[\"$\",\"figure\",\"67\",{\"children\":[\"\\n        \",[\"$\",\"$L11\",\"1\",{\"src\":\"https://cdn.math.computer/2025/1/8/hist_data.jpg\",\"alt\":\"\",\"width\":800,\"height\":600,\"className\":\"w-auto h-auto rounded-md mx-auto min-w-[min(640px,100%)]\",\"sizes\":\"800px\",\"quality\":78}],\"\\n        \",[\"$\",\"figcaption\",\"3\",{\"className\":\"text-center text-sm text-gray-600 dark:text-gray-400\",\"children\":\"Fig. 1b: Histograms of 2D embeddings based on Kosmos-2 annotations\"}],\"\\n      \"]}],[\"$\",\"p\",\"68\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"70\",{\"children\":\"\\n\"}],[\"$\",\"figure\",\"71\",{\"children\":[\"\\n        \",[\"$\",\"$L11\",\"1\",{\"src\":\"https://cdn.math.computer/2025/1/8/hist_train.jpg\",\"alt\":\"\",\"width\":800,\"height\":600,\"className\":\"w-auto h-auto rounded-md mx-auto min-w-[min(640px,100%)]\",\"sizes\":\"800px\",\"quality\":78}],\"\\n        \",[\"$\",\"figcaption\",\"3\",{\"className\":\"text-center text-sm text-gray-600 dark:text-gray-400\",\"children\":\"Fig. 2b: Histograms of 2D embeddedings based on human labels\"}],\"\\n      \"]}],[\"$\",\"p\",\"72\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"h2\",\"74\",{\"children\":\"Takeaways\"}],\"\\n\",[\"$\",\"p\",\"76\",{\"children\":[\"Stepping outside of art for a moment, I think this exercise revealed to me that when using dimension reduction models for exploratory data analysis, it really helps to separate the \",[\"$\",\"code\",\"1\",{\"className\":\"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm\",\"children\":\"fit\"}],\" and \",[\"$\",\"code\",\"3\",{\"className\":\"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm\",\"children\":\"transform\"}],\" steps out, using different datasets for each.\"]}],\"\\n\",[\"$\",\"p\",\"78\",{\"children\":[\"Using \",[\"$\",\"code\",\"1\",{\"className\":\"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm\",\"children\":\"fit_transform\"}],\", in essence, is a fine exercise to get a quick glance at your data.\\nBut when it comes to really understanding what something means, it behooves the practitioner to train a dimension-reduction model on a diverse and representative dataset of interest, and re-use it for inference.\"]}],\"\\n\",[\"$\",\"p\",\"80\",{\"children\":[\"However, if comparing multiple models upstream of the dimension-reduction process (e.g., swapping out \",[\"$\",\"code\",\"1\",{\"className\":\"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm\",\"children\":\"Kosmos-2\"}],\" or \",[\"$\",\"code\",\"3\",{\"className\":\"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm\",\"children\":\"nomic-ai/nomic-embed-text-v1\"}],\"), then the \",[\"$\",\"code\",\"5\",{\"className\":\"bg-neutral-100 dark:bg-neutral-800 px-1.5 py-0.5 rounded text-sm\",\"children\":\"fit_transform\"}],\" approach would be helpful for defining a metric to measure temporal stability.\\nGiven a fixed seed, and multiple models, the ones that travel a smaller distance are “more stable.”\"]}],\"\\n\",[\"$\",\"p\",\"82\",{\"children\":\"Instead of temporal stability, one can apply transformations to images that are either imperceptible to humans (e.g., small amounts of random noise), or would not impact their interpetation of the scene (e.g., a color tint).\\nThe same travel-distance metric would be appropriate for comparing those experiments to one another, and would work for both freshly-fit-with-same-seed and pre-trained reduction models.\"}],\"\\n\",[\"$\",\"p\",\"84\",{\"children\":[\"For me - when it comes to interpreting text annotations of video frames, I’m going to be sticking to using models I train on large quantities of data.\\nHowever, combining the two visuals as I did \",[\"$\",\"a\",\"1\",{\"href\":\"/1/7/\",\"children\":\"yesterday\"}],\" is also a viable option that shows an interesting contrast.\"]}],\"\\n\",[\"$\",\"h2\",\"86\",{\"children\":\"Next Steps\"}],\"\\n\",[\"$\",\"p\",\"88\",{\"children\":\"I’m going to repeat this process of generating images for several seeds, to study the sensitivity of the dimension-reduction model, and explore the visual diversity of using this particular framework.\"}],\"\\n\",[\"$\",\"p\",\"90\",{\"children\":\"Then I’ll move on to comparing several dimension-reduction algorithms on synthetic data to really understand (in a controlled setting), what visual phenomenon are exhibited by the different frameworks.\"}],\"\\n\"]}],[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 md:grid-cols-2 gap-4\",\"children\":[]}]]}]}]}],[\"$\",\"nav\",null,{\"className\":\"flex justify-between w-full mt-8 mb-4 text-3xl font-bold text-gray-600 dark:text-gray-400 px-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex-1 flex justify-start\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"/1/7\",\"className\":\"hover:underline\",\"children\":\"←\"}]}],[\"$\",\"div\",null,{\"className\":\"flex-1 flex justify-end\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"/1/9\",\"className\":\"hover:underline\",\"children\":\"→\"}]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"MathematicalMichaelx2025\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Digital Art Log for Dr. Michael Pilosov\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/2025/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"48x48\"}]]\n3:null\n"])</script></body></html>