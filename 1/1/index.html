<!DOCTYPE html><html lang="en" class="h-full"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="https://cdn.math.computer/v/kosmos2/fish/sm/frame_000001.png" fetchPriority="high"/><link rel="stylesheet" href="/2025/_next/static/css/df8c932e539912be.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/2025/_next/static/chunks/webpack-b2ce682491b35475.js"/><script src="/2025/_next/static/chunks/fd9d1056-a86d52f6b747304e.js" async=""></script><script src="/2025/_next/static/chunks/117-003a6b1a715a95e6.js" async=""></script><script src="/2025/_next/static/chunks/main-app-3c13ae280b9991a1.js" async=""></script><script src="/2025/_next/static/chunks/app/layout-c0991f7d76273a13.js" async=""></script><script src="/2025/_next/static/chunks/450-fc8431448c3438bc.js" async=""></script><script src="/2025/_next/static/chunks/878-5d3156bd3161bc5d.js" async=""></script><script src="/2025/_next/static/chunks/app/%5Bmonth%5D/%5Bday%5D/page-475f15d3cd396c50.js" async=""></script><title>MathematicalMichaelx2025</title><meta name="description" content="Digital Art Log for Dr. Michael Pilosov"/><link rel="icon" href="/2025/favicon.ico" type="image/x-icon" sizes="48x48"/><script src="/2025/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="min-h-full bg-white dark:bg-black"><script>((e,t,r,n,l,o,a,u)=>{let i=document.documentElement,c=["light","dark"];function d(t){(Array.isArray(e)?e:[e]).forEach(e=>{let r="class"===e,n=r&&o?l.map(e=>o[e]||e):l;r?(i.classList.remove(...n),i.classList.add(t)):i.setAttribute(e,t)}),u&&c.includes(t)&&(i.style.colorScheme=t)}if(n)d(n);else try{let e=localStorage.getItem(t)||r,n=a&&"system"===e?window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light":e;d(n)}catch(e){}})("class","theme","system",null,["light","dark"],null,true,true)</script><div class="flex flex-col min-h-full"><div class="flex-1 w-full"><div class="min-h-screen bg-white dark:bg-black text-black dark:text-white"><header class="flex justify-between items-center mb-8 px-4"><a class="text-2xl font-bold hover:underline" href="/2025/">2025 (12 Events)</a><div class="flex items-center"><a class="hover:underline mr-4" href="/2025/about/">About</a><button class="inline-flex items-center justify-center gap-2 whitespace-nowrap rounded-md text-sm font-medium transition-colors focus-visible:outline-none focus-visible:ring-1 focus-visible:ring-ring disabled:pointer-events-none disabled:opacity-50 [&amp;_svg]:pointer-events-none [&amp;_svg]:size-4 [&amp;_svg]:shrink-0 hover:text-accent-foreground h-9 hover:bg-transparent p-0" disabled=""><div class="h-5 w-5"></div><span class="sr-only">Loading theme toggle</span></button></div></header><nav class="flex justify-between w-full mb-8 text-3xl font-bold text-gray-600 dark:text-gray-400 px-4"><div class="flex-1 flex justify-start"></div><div class="flex-1 flex justify-end"><a class="hover:underline" href="/2025/1/2/">→</a></div></nav><main><div class="max-w-content mx-auto px-4"><article><h1 class="text-3xl font-bold mb-2">Initial Experiments in Image Understanding</h1><p class="text-gray-600 dark:text-gray-400 mb-4">2025-01-01</p><div class="prose dark:prose-invert mb-8 [&amp;&gt;p]:mb-4 [&amp;_a]:underline [&amp;_a:hover]:text-gray-600 dark:[&amp;_a:hover]:text-gray-300 [&amp;&gt;ul]:list-disc [&amp;&gt;ul]:pl-6 [&amp;&gt;ul]:mb-4 [&amp;&gt;ul&gt;li]:pl-2"><h2>Context</h2>
<p>I have long been interested in how machines perceive the world.
We are told so much about “artificial intelligence” and are impressed by new capabilities, but I have always had my reservations and suspicions that there was more pattern-matching going on, and less understanding.</p>
<p>Speculations on whether the current architectures will end up being the path forward is not in scope for this first post.</p>
<p>One thing is for certain: the capabilities demonstrated by language models in 2024 were incredible leaps forward.
The resources required for leveraging these best-performing models are prohibitive, but I have explored the functionality of open / permissive models, and thought a lot about how to incorporate them into my artistic practice.</p>
<p>To date, I have not finalized any projects that leveraged language models, but I have been studying the relationship between what Machine Learning models “learn” and consequently how they perceive / interpret the world.
One of my prior <a href="https://hues.mpilosov.com">projects</a> interrogated this question through the lens of arranging colors.</p>
<p>This year, I want to actualize my experiments with language models, and turn some sketches of ideas into something visually compelling that I can print or animate.</p>
<h2>A Model of Interest</h2>
<p>Recently I came across <a href="https://arxiv.org/abs/2306.14824">Kosmos-2</a>, a “Multimodal Large Language Model (MLLM)” that enables new capabilities of perceiving object descriptions (e.g., bounding boxes) and “grounding text” to the visual world.</p>
<p>What the authors mean by “grounding text” is that the model can answer questions about the image and cite its answers as part of its response in a structured format.
In essence, this is the textual equivalent of a computer “pointing” to things in a picture, citing its answers.</p>
<p>Models that draw bounding boxes and recognize objects in images have long existed, as have models that can provide a description (caption) of an image.
What makes this one different is that it is providing both at the same time, using the transformer architecture which has led to the recent AI / LLM developments in the headlines.</p>
<p>This model (unlike those that caption images), allows for free-form questions, meaning they can be explored and interacted with through “prompt engineering.”</p>
<h2>Artistic Inspiration</h2>
<p>This model appeals to me for a few potential use cases.
I like that it can answer questions about the world and ground responses in object detections.</p>
<p>I can imagine a system that watches the world and tries to explain it in real-time, while we humans watch its understanding of the world evolve in real-time through some visualization technique.</p>
<p>I want to see what this model thinks is in the photographs I take.</p>
<p>Moreover, I am interested in having conversations with this model about the compositions and aesthetics of my abstract art, a task which was surely outside the scope of its training regiment, but one that I can hopefully hack together.</p>
<h2>First Steps</h2>
<p>I needed to download the model from HuggingFace, following the documentation in Microsoft’s github repository.
It was pretty easy to get inference working, and after going through a handful of their examples, I was ready to start using my own images and began refactoring the inference code for distributed compute.</p>
<p>One thing I do often is split video into still-frames, process them, and stitch them back together.
Given that this model takes an image as input, this workflow seemed quite appropriate to try.</p>
<h2>Early Trials</h2>
<p>First, here is a ~30s video clip I took on my phone while visiting an aquarium.</p>
<p>
</p><figure>
        <video controls="">
          <source src="https://cdn.math.computer/v/kosmos2/fish/sm/input.mp4" type="video/mp4"/>
          Your browser does not support the video tag.
        </video>
        <figcaption class="text-center text-sm text-gray-600 dark:text-gray-400">Original video, taken at Seattle Aquarium, Dec 2024. Scaled to 1/3 resolution.</figcaption>
      </figure><p></p>
<p>I absolutely fell in love watching this fish tunnel a home for itself underneath a coral.</p>
<p>For some reason, this clip came to mind as something interesting to run through the model.</p>
<ul>
<li>There’s a diversity of things going on but not <em>too</em> much.</li>
<li>It should be clearly recognizable as an aquarium, which should ground the context of answers to some degree (for example, it would be reasonable to misidentify a fish for an eel but not a dog)</li>
<li>There are varying levels of zoom, which changes the frame of what is visible in the scene.</li>
<li>Something “interesting” is happening in it - but the behavior is observed over time. Trying to identify it from any given frame would be challenging.</li>
<li>It’s calming to look at, which is important for me during testing algorithms since I’ll be reviewing the footage a lot.</li>
</ul>
<p>I first tried a generic “Describe this scene:” prompt, but on individual frames I found that asking for detail helped increase the number of detections and length of description.</p>
<p>
</p><figure>
        <video controls="">
          <source src="https://cdn.math.computer/v/kosmos2/fish/sm/detail-scene-rich-vocab.mp4" type="video/mp4"/>
          Your browser does not support the video tag.
        </video>
        <figcaption class="text-center text-sm text-gray-600 dark:text-gray-400">Video processed with prompt \</figcaption>
      </figure><p></p>
<p>I tried “Find the white fish” which did help keep track of the fish a bit, too.</p>
<h3>Observations:</h3>
<ul>
<li>The burrowing behavior was not recognized.</li>
<li>Temporal stability of scene descriptions was surprisingly good at first glance.</li>
<li>Processing more than a handful of frames per second on my GPU hardware is not possible. If I want real-time annotations, it will require <em>a lot</em> of parallel compute.</li>
</ul>
<p>Overall, this was an interesting initial set of experiments.
I’ll continue to iterate with this model - I think it has potential for some interesting visuals.</p>
<p>My iteration cycle needs to be short so that I can stay in a “flow state” when making art, and right now processing images with Kosmos-2 is <em>much too slow</em> for anything pertaining to video.</p>
<p>So while it did detract from playing with the model in more artistic ways, I built and published an API version of the model, which will be required to scale my experiments.
Can I achieve something approaching real-time processing on a budget?</p>
<p>More on that soon.</p>
</div><div class="grid grid-cols-1 md:grid-cols-2 gap-4"><figure class="flex flex-col items-center"><div class="relative overflow-hidden"><img alt="Annotated Frame 1" fetchPriority="high" loading="eager" width="960" height="540" decoding="async" data-nimg="1" class="w-max-full" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 960 540&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjcwMCIgaGVpZ2h0PSI0NzUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgPGRlZnM+CiAgICA8bGluZWFyR3JhZGllbnQgaWQ9ImciPgogICAgICA8c3RvcCBzdG9wLWNvbG9yPSIjMDAwMCIgb2Zmc2V0PSIyMCUiIC8+CiAgICAgIDxzdG9wIHN0b3AtY29sb3I9IiMwMDAxIiBvZmZzZXQ9IjUwJSIgLz4KICAgICAgPHN0b3Agc3RvcC1jb2xvcj0iIzAwMDAiIG9mZnNldD0iNzAlIiAvPgogICAgPC9saW5lYXJHcmFkaWVudD4KICA8L2RlZnM+CiAgPHJlY3Qgd2lkdGg9IjcwMCIgaGVpZ2h0PSI0NzUiIGZpbGw9IiMwMDAwIiAvPgogIDxyZWN0IGlkPSJyIiB3aWR0aD0iNzAwIiBoZWlnaHQ9IjQ3NSIgZmlsbD0idXJsKCNnKSIgLz4KICA8YW5pbWF0ZSB4bGluazpocmVmPSIjciIgYXR0cmlidXRlTmFtZT0ieCIgZnJvbT0iLTcwMCIgdG89IjcwMCIgZHVyPSIxcyIgcmVwZWF0Q291bnQ9ImluZGVmaW5pdGUiICAvPgo8L3N2Zz4=&#x27;/%3E%3C/svg%3E&quot;)" src="https://cdn.math.computer/v/kosmos2/fish/sm/frame_000001.png"/></div><figcaption class="mt-2 text-center text-sm text-gray-600 dark:text-gray-400">Kosmos-2 result for frame 1</figcaption></figure><figure class="flex flex-col items-center"><div class="relative overflow-hidden"><img alt="Annotated Frame 10" loading="lazy" width="960" height="540" decoding="async" data-nimg="1" class="w-max-full" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 960 540&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjcwMCIgaGVpZ2h0PSI0NzUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgPGRlZnM+CiAgICA8bGluZWFyR3JhZGllbnQgaWQ9ImciPgogICAgICA8c3RvcCBzdG9wLWNvbG9yPSIjMDAwMCIgb2Zmc2V0PSIyMCUiIC8+CiAgICAgIDxzdG9wIHN0b3AtY29sb3I9IiMwMDAxIiBvZmZzZXQ9IjUwJSIgLz4KICAgICAgPHN0b3Agc3RvcC1jb2xvcj0iIzAwMDAiIG9mZnNldD0iNzAlIiAvPgogICAgPC9saW5lYXJHcmFkaWVudD4KICA8L2RlZnM+CiAgPHJlY3Qgd2lkdGg9IjcwMCIgaGVpZ2h0PSI0NzUiIGZpbGw9IiMwMDAwIiAvPgogIDxyZWN0IGlkPSJyIiB3aWR0aD0iNzAwIiBoZWlnaHQ9IjQ3NSIgZmlsbD0idXJsKCNnKSIgLz4KICA8YW5pbWF0ZSB4bGluazpocmVmPSIjciIgYXR0cmlidXRlTmFtZT0ieCIgZnJvbT0iLTcwMCIgdG89IjcwMCIgZHVyPSIxcyIgcmVwZWF0Q291bnQ9ImluZGVmaW5pdGUiICAvPgo8L3N2Zz4=&#x27;/%3E%3C/svg%3E&quot;)" src="https://cdn.math.computer/v/kosmos2/fish/sm/frame_000010.png"/></div><figcaption class="mt-2 text-center text-sm text-gray-600 dark:text-gray-400">Kosmos-2 result for frame 10</figcaption></figure><figure class="flex flex-col items-center"><div class="relative overflow-hidden"><img alt="Annotated Frame 100" loading="lazy" width="960" height="540" decoding="async" data-nimg="1" class="w-max-full" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 960 540&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjcwMCIgaGVpZ2h0PSI0NzUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgPGRlZnM+CiAgICA8bGluZWFyR3JhZGllbnQgaWQ9ImciPgogICAgICA8c3RvcCBzdG9wLWNvbG9yPSIjMDAwMCIgb2Zmc2V0PSIyMCUiIC8+CiAgICAgIDxzdG9wIHN0b3AtY29sb3I9IiMwMDAxIiBvZmZzZXQ9IjUwJSIgLz4KICAgICAgPHN0b3Agc3RvcC1jb2xvcj0iIzAwMDAiIG9mZnNldD0iNzAlIiAvPgogICAgPC9saW5lYXJHcmFkaWVudD4KICA8L2RlZnM+CiAgPHJlY3Qgd2lkdGg9IjcwMCIgaGVpZ2h0PSI0NzUiIGZpbGw9IiMwMDAwIiAvPgogIDxyZWN0IGlkPSJyIiB3aWR0aD0iNzAwIiBoZWlnaHQ9IjQ3NSIgZmlsbD0idXJsKCNnKSIgLz4KICA8YW5pbWF0ZSB4bGluazpocmVmPSIjciIgYXR0cmlidXRlTmFtZT0ieCIgZnJvbT0iLTcwMCIgdG89IjcwMCIgZHVyPSIxcyIgcmVwZWF0Q291bnQ9ImluZGVmaW5pdGUiICAvPgo8L3N2Zz4=&#x27;/%3E%3C/svg%3E&quot;)" src="https://cdn.math.computer/v/kosmos2/fish/sm/frame_000100.png"/></div><figcaption class="mt-2 text-center text-sm text-gray-600 dark:text-gray-400">Kosmos-2 result for frame 100</figcaption></figure><figure class="flex flex-col items-center"><div class="relative overflow-hidden"><img alt="Annotated Frame 1000" loading="lazy" width="960" height="540" decoding="async" data-nimg="1" class="w-max-full" style="color:transparent;background-size:cover;background-position:50% 50%;background-repeat:no-repeat;background-image:url(&quot;data:image/svg+xml;charset=utf-8,%3Csvg xmlns=&#x27;http://www.w3.org/2000/svg&#x27; viewBox=&#x27;0 0 960 540&#x27;%3E%3Cfilter id=&#x27;b&#x27; color-interpolation-filters=&#x27;sRGB&#x27;%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3CfeColorMatrix values=&#x27;1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 100 -1&#x27; result=&#x27;s&#x27;/%3E%3CfeFlood x=&#x27;0&#x27; y=&#x27;0&#x27; width=&#x27;100%25&#x27; height=&#x27;100%25&#x27;/%3E%3CfeComposite operator=&#x27;out&#x27; in=&#x27;s&#x27;/%3E%3CfeComposite in2=&#x27;SourceGraphic&#x27;/%3E%3CfeGaussianBlur stdDeviation=&#x27;20&#x27;/%3E%3C/filter%3E%3Cimage width=&#x27;100%25&#x27; height=&#x27;100%25&#x27; x=&#x27;0&#x27; y=&#x27;0&#x27; preserveAspectRatio=&#x27;none&#x27; style=&#x27;filter: url(%23b);&#x27; href=&#x27;data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjcwMCIgaGVpZ2h0PSI0NzUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgPGRlZnM+CiAgICA8bGluZWFyR3JhZGllbnQgaWQ9ImciPgogICAgICA8c3RvcCBzdG9wLWNvbG9yPSIjMDAwMCIgb2Zmc2V0PSIyMCUiIC8+CiAgICAgIDxzdG9wIHN0b3AtY29sb3I9IiMwMDAxIiBvZmZzZXQ9IjUwJSIgLz4KICAgICAgPHN0b3Agc3RvcC1jb2xvcj0iIzAwMDAiIG9mZnNldD0iNzAlIiAvPgogICAgPC9saW5lYXJHcmFkaWVudD4KICA8L2RlZnM+CiAgPHJlY3Qgd2lkdGg9IjcwMCIgaGVpZ2h0PSI0NzUiIGZpbGw9IiMwMDAwIiAvPgogIDxyZWN0IGlkPSJyIiB3aWR0aD0iNzAwIiBoZWlnaHQ9IjQ3NSIgZmlsbD0idXJsKCNnKSIgLz4KICA8YW5pbWF0ZSB4bGluazpocmVmPSIjciIgYXR0cmlidXRlTmFtZT0ieCIgZnJvbT0iLTcwMCIgdG89IjcwMCIgZHVyPSIxcyIgcmVwZWF0Q291bnQ9ImluZGVmaW5pdGUiICAvPgo8L3N2Zz4=&#x27;/%3E%3C/svg%3E&quot;)" src="https://cdn.math.computer/v/kosmos2/fish/sm/frame_001000.png"/></div><figcaption class="mt-2 text-center text-sm text-gray-600 dark:text-gray-400">Kosmos-2 result for frame 1000</figcaption></figure></div></article></div></main><nav class="flex justify-between w-full mt-8 mb-4 text-3xl font-bold text-gray-600 dark:text-gray-400 px-4"><div class="flex-1 flex justify-start"></div><div class="flex-1 flex justify-end"><a class="hover:underline" href="/2025/1/2/">→</a></div></nav></div></div><footer class="text-center text-sm text-gray-600 dark:text-gray-400 py-4 bg-white dark:bg-black">© <!-- -->2025<!-- --> Michael Pilosov. All rights reserved.</footer></div><script src="/2025/_next/static/chunks/webpack-b2ce682491b35475.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/2025/_next/static/css/df8c932e539912be.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"2:I[2846,[],\"\"]\n5:I[4707,[],\"\"]\n8:I[6423,[],\"\"]\n9:I[2798,[\"185\",\"static/chunks/app/layout-c0991f7d76273a13.js\"],\"ThemeProvider\"]\na:I[9734,[\"185\",\"static/chunks/app/layout-c0991f7d76273a13.js\"],\"DynamicFavicon\"]\nb:I[8291,[\"185\",\"static/chunks/app/layout-c0991f7d76273a13.js\"],\"Analytics\"]\nd:I[1060,[],\"\"]\n6:[\"month\",\"1\",\"d\"]\n7:[\"day\",\"1\",\"d\"]\ne:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L2\",null,{\"buildId\":\"VQoYmXv-G56ZgJRRzJJFi\",\"assetPrefix\":\"/2025\",\"urlParts\":[\"\",\"1\",\"1\",\"\"],\"initialTree\":[\"\",{\"children\":[[\"month\",\"1\",\"d\"],{\"children\":[[\"day\",\"1\",\"d\"],{\"children\":[\"__PAGE__?{\\\"month\\\":\\\"1\\\",\\\"day\\\":\\\"1\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[[\"month\",\"1\",\"d\"],{\"children\":[[\"day\",\"1\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$L3\",\"$L4\",null],null],null]},[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"$6\",\"children\",\"$7\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"$6\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/2025/_next/static/css/df8c932e539912be.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"suppressHydrationWarning\":true,\"className\":\"h-full\",\"children\":[\"$\",\"body\",null,{\"className\":\"min-h-full bg-white dark:bg-black\",\"children\":[[\"$\",\"$L9\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col min-h-full\",\"children\":[[\"$\",\"$La\",null,{}],[\"$\",\"div\",null,{\"className\":\"flex-1 w-full\",\"children\":[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[]}]}],[\"$\",\"footer\",null,{\"className\":\"text-center text-sm text-gray-600 dark:text-gray-400 py-4 bg-white dark:bg-black\",\"children\":[\"© \",2025,\" Michael Pilosov. All rights reserved.\"]}]]}]}],[\"$\",\"$Lb\",null,{}]]}]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$Lc\"],\"globalErrorComponent\":\"$d\",\"missingSlots\":\"$We\"}]\n"])</script><script>self.__next_f.push([1,"f:I[2972,[\"450\",\"static/chunks/450-fc8431448c3438bc.js\",\"878\",\"static/chunks/878-5d3156bd3161bc5d.js\",\"160\",\"static/chunks/app/%5Bmonth%5D/%5Bday%5D/page-475f15d3cd396c50.js\"],\"\"]\n10:I[1190,[\"450\",\"static/chunks/450-fc8431448c3438bc.js\",\"878\",\"static/chunks/878-5d3156bd3161bc5d.js\",\"160\",\"static/chunks/app/%5Bmonth%5D/%5Bday%5D/page-475f15d3cd396c50.js\"],\"ThemeToggle\"]\n11:I[5878,[\"450\",\"static/chunks/450-fc8431448c3438bc.js\",\"878\",\"static/chunks/878-5d3156bd3161bc5d.js\",\"160\",\"static/chunks/app/%5Bmonth%5D/%5Bday%5D/page-475f15d3cd396c50.js\"],\"Image\"]\n"])</script><script>self.__next_f.push([1,"4:[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-white dark:bg-black text-black dark:text-white\",\"children\":[[\"$\",\"header\",null,{\"className\":\"flex justify-between items-center mb-8 px-4\",\"children\":[[\"$\",\"$Lf\",null,{\"href\":\"/\",\"className\":\"text-2xl font-bold hover:underline\",\"children\":\"2025 (12 Events)\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center\",\"children\":[[\"$\",\"$Lf\",null,{\"href\":\"/about\",\"className\":\"hover:underline mr-4\",\"children\":\"About\"}],[\"$\",\"$L10\",null,{}]]}]]}],[\"$\",\"nav\",null,{\"className\":\"flex justify-between w-full mb-8 text-3xl font-bold text-gray-600 dark:text-gray-400 px-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex-1 flex justify-start\",\"children\":null}],[\"$\",\"div\",null,{\"className\":\"flex-1 flex justify-end\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"/1/2\",\"className\":\"hover:underline\",\"children\":\"→\"}]}]]}],[\"$\",\"main\",null,{\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-content mx-auto px-4\",\"children\":[\"$\",\"article\",null,{\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-3xl font-bold mb-2\",\"children\":\"Initial Experiments in Image Understanding\"}],[\"$\",\"p\",null,{\"className\":\"text-gray-600 dark:text-gray-400 mb-4\",\"children\":\"2025-01-01\"}],[\"$\",\"div\",null,{\"className\":\"prose dark:prose-invert mb-8 [\u0026\u003ep]:mb-4 [\u0026_a]:underline [\u0026_a:hover]:text-gray-600 dark:[\u0026_a:hover]:text-gray-300 [\u0026\u003eul]:list-disc [\u0026\u003eul]:pl-6 [\u0026\u003eul]:mb-4 [\u0026\u003eul\u003eli]:pl-2\",\"children\":[[\"$\",\"h2\",\"0\",{\"children\":\"Context\"}],\"\\n\",[\"$\",\"p\",\"2\",{\"children\":\"I have long been interested in how machines perceive the world.\\nWe are told so much about “artificial intelligence” and are impressed by new capabilities, but I have always had my reservations and suspicions that there was more pattern-matching going on, and less understanding.\"}],\"\\n\",[\"$\",\"p\",\"4\",{\"children\":\"Speculations on whether the current architectures will end up being the path forward is not in scope for this first post.\"}],\"\\n\",[\"$\",\"p\",\"6\",{\"children\":\"One thing is for certain: the capabilities demonstrated by language models in 2024 were incredible leaps forward.\\nThe resources required for leveraging these best-performing models are prohibitive, but I have explored the functionality of open / permissive models, and thought a lot about how to incorporate them into my artistic practice.\"}],\"\\n\",[\"$\",\"p\",\"8\",{\"children\":[\"To date, I have not finalized any projects that leveraged language models, but I have been studying the relationship between what Machine Learning models “learn” and consequently how they perceive / interpret the world.\\nOne of my prior \",[\"$\",\"a\",\"1\",{\"href\":\"https://hues.mpilosov.com\",\"children\":\"projects\"}],\" interrogated this question through the lens of arranging colors.\"]}],\"\\n\",[\"$\",\"p\",\"10\",{\"children\":\"This year, I want to actualize my experiments with language models, and turn some sketches of ideas into something visually compelling that I can print or animate.\"}],\"\\n\",[\"$\",\"h2\",\"12\",{\"children\":\"A Model of Interest\"}],\"\\n\",[\"$\",\"p\",\"14\",{\"children\":[\"Recently I came across \",[\"$\",\"a\",\"1\",{\"href\":\"https://arxiv.org/abs/2306.14824\",\"children\":\"Kosmos-2\"}],\", a “Multimodal Large Language Model (MLLM)” that enables new capabilities of perceiving object descriptions (e.g., bounding boxes) and “grounding text” to the visual world.\"]}],\"\\n\",[\"$\",\"p\",\"16\",{\"children\":\"What the authors mean by “grounding text” is that the model can answer questions about the image and cite its answers as part of its response in a structured format.\\nIn essence, this is the textual equivalent of a computer “pointing” to things in a picture, citing its answers.\"}],\"\\n\",[\"$\",\"p\",\"18\",{\"children\":\"Models that draw bounding boxes and recognize objects in images have long existed, as have models that can provide a description (caption) of an image.\\nWhat makes this one different is that it is providing both at the same time, using the transformer architecture which has led to the recent AI / LLM developments in the headlines.\"}],\"\\n\",[\"$\",\"p\",\"20\",{\"children\":\"This model (unlike those that caption images), allows for free-form questions, meaning they can be explored and interacted with through “prompt engineering.”\"}],\"\\n\",[\"$\",\"h2\",\"22\",{\"children\":\"Artistic Inspiration\"}],\"\\n\",[\"$\",\"p\",\"24\",{\"children\":\"This model appeals to me for a few potential use cases.\\nI like that it can answer questions about the world and ground responses in object detections.\"}],\"\\n\",[\"$\",\"p\",\"26\",{\"children\":\"I can imagine a system that watches the world and tries to explain it in real-time, while we humans watch its understanding of the world evolve in real-time through some visualization technique.\"}],\"\\n\",[\"$\",\"p\",\"28\",{\"children\":\"I want to see what this model thinks is in the photographs I take.\"}],\"\\n\",[\"$\",\"p\",\"30\",{\"children\":\"Moreover, I am interested in having conversations with this model about the compositions and aesthetics of my abstract art, a task which was surely outside the scope of its training regiment, but one that I can hopefully hack together.\"}],\"\\n\",[\"$\",\"h2\",\"32\",{\"children\":\"First Steps\"}],\"\\n\",[\"$\",\"p\",\"34\",{\"children\":\"I needed to download the model from HuggingFace, following the documentation in Microsoft’s github repository.\\nIt was pretty easy to get inference working, and after going through a handful of their examples, I was ready to start using my own images and began refactoring the inference code for distributed compute.\"}],\"\\n\",[\"$\",\"p\",\"36\",{\"children\":\"One thing I do often is split video into still-frames, process them, and stitch them back together.\\nGiven that this model takes an image as input, this workflow seemed quite appropriate to try.\"}],\"\\n\",[\"$\",\"h2\",\"38\",{\"children\":\"Early Trials\"}],\"\\n\",[\"$\",\"p\",\"40\",{\"children\":\"First, here is a ~30s video clip I took on my phone while visiting an aquarium.\"}],\"\\n\",[\"$\",\"p\",\"42\",{\"children\":\"\\n\"}],[\"$\",\"figure\",\"43\",{\"children\":[\"\\n        \",[\"$\",\"video\",\"1\",{\"controls\":true,\"children\":[\"\\n          \",[\"$\",\"source\",\"1\",{\"src\":\"https://cdn.math.computer/v/kosmos2/fish/sm/input.mp4\",\"type\":\"video/mp4\",\"children\":\"$undefined\"}],\"\\n          Your browser does not support the video tag.\\n        \"]}],\"\\n        \",[\"$\",\"figcaption\",\"3\",{\"className\":\"text-center text-sm text-gray-600 dark:text-gray-400\",\"children\":\"Original video, taken at Seattle Aquarium, Dec 2024. Scaled to 1/3 resolution.\"}],\"\\n      \"]}],[\"$\",\"p\",\"44\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"46\",{\"children\":\"I absolutely fell in love watching this fish tunnel a home for itself underneath a coral.\"}],\"\\n\",[\"$\",\"p\",\"48\",{\"children\":\"For some reason, this clip came to mind as something interesting to run through the model.\"}],\"\\n\",[\"$\",\"ul\",\"50\",{\"children\":[\"\\n\",[\"$\",\"li\",\"1\",{\"children\":[\"There’s a diversity of things going on but not \",[\"$\",\"em\",\"1\",{\"children\":\"too\"}],\" much.\"]}],\"\\n\",[\"$\",\"li\",\"3\",{\"children\":\"It should be clearly recognizable as an aquarium, which should ground the context of answers to some degree (for example, it would be reasonable to misidentify a fish for an eel but not a dog)\"}],\"\\n\",[\"$\",\"li\",\"5\",{\"children\":\"There are varying levels of zoom, which changes the frame of what is visible in the scene.\"}],\"\\n\",[\"$\",\"li\",\"7\",{\"children\":\"Something “interesting” is happening in it - but the behavior is observed over time. Trying to identify it from any given frame would be challenging.\"}],\"\\n\",[\"$\",\"li\",\"9\",{\"children\":\"It’s calming to look at, which is important for me during testing algorithms since I’ll be reviewing the footage a lot.\"}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",\"52\",{\"children\":\"I first tried a generic “Describe this scene:” prompt, but on individual frames I found that asking for detail helped increase the number of detections and length of description.\"}],\"\\n\",[\"$\",\"p\",\"54\",{\"children\":\"\\n\"}],[\"$\",\"figure\",\"55\",{\"children\":[\"\\n        \",[\"$\",\"video\",\"1\",{\"controls\":true,\"children\":[\"\\n          \",[\"$\",\"source\",\"1\",{\"src\":\"https://cdn.math.computer/v/kosmos2/fish/sm/detail-scene-rich-vocab.mp4\",\"type\":\"video/mp4\",\"children\":\"$undefined\"}],\"\\n          Your browser does not support the video tag.\\n        \"]}],\"\\n        \",[\"$\",\"figcaption\",\"3\",{\"className\":\"text-center text-sm text-gray-600 dark:text-gray-400\",\"children\":\"Video processed with prompt \\\\\"}],\"\\n      \"]}],[\"$\",\"p\",\"56\",{\"children\":\"$undefined\"}],\"\\n\",[\"$\",\"p\",\"58\",{\"children\":\"I tried “Find the white fish” which did help keep track of the fish a bit, too.\"}],\"\\n\",[\"$\",\"h3\",\"60\",{\"children\":\"Observations:\"}],\"\\n\",[\"$\",\"ul\",\"62\",{\"children\":[\"\\n\",[\"$\",\"li\",\"1\",{\"children\":\"The burrowing behavior was not recognized.\"}],\"\\n\",[\"$\",\"li\",\"3\",{\"children\":\"Temporal stability of scene descriptions was surprisingly good at first glance.\"}],\"\\n\",[\"$\",\"li\",\"5\",{\"children\":[\"Processing more than a handful of frames per second on my GPU hardware is not possible. If I want real-time annotations, it will require \",[\"$\",\"em\",\"1\",{\"children\":\"a lot\"}],\" of parallel compute.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",\"64\",{\"children\":\"Overall, this was an interesting initial set of experiments.\\nI’ll continue to iterate with this model - I think it has potential for some interesting visuals.\"}],\"\\n\",[\"$\",\"p\",\"66\",{\"children\":[\"My iteration cycle needs to be short so that I can stay in a “flow state” when making art, and right now processing images with Kosmos-2 is \",[\"$\",\"em\",\"1\",{\"children\":\"much too slow\"}],\" for anything pertaining to video.\"]}],\"\\n\",[\"$\",\"p\",\"68\",{\"children\":\"So while it did detract from playing with the model in more artistic ways, I built and published an API version of the model, which will be required to scale my experiments.\\nCan I achieve something approaching real-time processing on a budget?\"}],\"\\n\",[\"$\",\"p\",\"70\",{\"children\":\"More on that soon.\"}],\"\\n\"]}],[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 md:grid-cols-2 gap-4\",\"children\":[[\"$\",\"figure\",\"0\",{\"className\":\"flex flex-col items-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"relative overflow-hidden\",\"children\":[\"$\",\"$L11\",null,{\"src\":\"https://cdn.math.computer/v/kosmos2/fish/sm/frame_000001.png\",\"alt\":\"Annotated Frame 1\",\"width\":960,\"height\":540,\"className\":\"w-max-full\",\"sizes\":\"960px\",\"quality\":78,\"priority\":true,\"loading\":\"eager\",\"placeholder\":\"blur\",\"blurDataURL\":\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjcwMCIgaGVpZ2h0PSI0NzUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgPGRlZnM+CiAgICA8bGluZWFyR3JhZGllbnQgaWQ9ImciPgogICAgICA8c3RvcCBzdG9wLWNvbG9yPSIjMDAwMCIgb2Zmc2V0PSIyMCUiIC8+CiAgICAgIDxzdG9wIHN0b3AtY29sb3I9IiMwMDAxIiBvZmZzZXQ9IjUwJSIgLz4KICAgICAgPHN0b3Agc3RvcC1jb2xvcj0iIzAwMDAiIG9mZnNldD0iNzAlIiAvPgogICAgPC9saW5lYXJHcmFkaWVudD4KICA8L2RlZnM+CiAgPHJlY3Qgd2lkdGg9IjcwMCIgaGVpZ2h0PSI0NzUiIGZpbGw9IiMwMDAwIiAvPgogIDxyZWN0IGlkPSJyIiB3aWR0aD0iNzAwIiBoZWlnaHQ9IjQ3NSIgZmlsbD0idXJsKCNnKSIgLz4KICA8YW5pbWF0ZSB4bGluazpocmVmPSIjciIgYXR0cmlidXRlTmFtZT0ieCIgZnJvbT0iLTcwMCIgdG89IjcwMCIgZHVyPSIxcyIgcmVwZWF0Q291bnQ9ImluZGVmaW5pdGUiICAvPgo8L3N2Zz4=\"}]}],[\"$\",\"figcaption\",null,{\"className\":\"mt-2 text-center text-sm text-gray-600 dark:text-gray-400\",\"dangerouslySetInnerHTML\":{\"__html\":\"Kosmos-2 result for frame 1\"}}]]}],[\"$\",\"figure\",\"1\",{\"className\":\"flex flex-col items-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"relative overflow-hidden\",\"children\":[\"$\",\"$L11\",null,{\"src\":\"https://cdn.math.computer/v/kosmos2/fish/sm/frame_000010.png\",\"alt\":\"Annotated Frame 10\",\"width\":960,\"height\":540,\"className\":\"w-max-full\",\"sizes\":\"960px\",\"quality\":78,\"priority\":false,\"loading\":\"lazy\",\"placeholder\":\"blur\",\"blurDataURL\":\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjcwMCIgaGVpZ2h0PSI0NzUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgPGRlZnM+CiAgICA8bGluZWFyR3JhZGllbnQgaWQ9ImciPgogICAgICA8c3RvcCBzdG9wLWNvbG9yPSIjMDAwMCIgb2Zmc2V0PSIyMCUiIC8+CiAgICAgIDxzdG9wIHN0b3AtY29sb3I9IiMwMDAxIiBvZmZzZXQ9IjUwJSIgLz4KICAgICAgPHN0b3Agc3RvcC1jb2xvcj0iIzAwMDAiIG9mZnNldD0iNzAlIiAvPgogICAgPC9saW5lYXJHcmFkaWVudD4KICA8L2RlZnM+CiAgPHJlY3Qgd2lkdGg9IjcwMCIgaGVpZ2h0PSI0NzUiIGZpbGw9IiMwMDAwIiAvPgogIDxyZWN0IGlkPSJyIiB3aWR0aD0iNzAwIiBoZWlnaHQ9IjQ3NSIgZmlsbD0idXJsKCNnKSIgLz4KICA8YW5pbWF0ZSB4bGluazpocmVmPSIjciIgYXR0cmlidXRlTmFtZT0ieCIgZnJvbT0iLTcwMCIgdG89IjcwMCIgZHVyPSIxcyIgcmVwZWF0Q291bnQ9ImluZGVmaW5pdGUiICAvPgo8L3N2Zz4=\"}]}],[\"$\",\"figcaption\",null,{\"className\":\"mt-2 text-center text-sm text-gray-600 dark:text-gray-400\",\"dangerouslySetInnerHTML\":{\"__html\":\"Kosmos-2 result for frame 10\"}}]]}],[\"$\",\"figure\",\"2\",{\"className\":\"flex flex-col items-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"relative overflow-hidden\",\"children\":[\"$\",\"$L11\",null,{\"src\":\"https://cdn.math.computer/v/kosmos2/fish/sm/frame_000100.png\",\"alt\":\"Annotated Frame 100\",\"width\":960,\"height\":540,\"className\":\"w-max-full\",\"sizes\":\"960px\",\"quality\":78,\"priority\":false,\"loading\":\"lazy\",\"placeholder\":\"blur\",\"blurDataURL\":\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjcwMCIgaGVpZ2h0PSI0NzUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgPGRlZnM+CiAgICA8bGluZWFyR3JhZGllbnQgaWQ9ImciPgogICAgICA8c3RvcCBzdG9wLWNvbG9yPSIjMDAwMCIgb2Zmc2V0PSIyMCUiIC8+CiAgICAgIDxzdG9wIHN0b3AtY29sb3I9IiMwMDAxIiBvZmZzZXQ9IjUwJSIgLz4KICAgICAgPHN0b3Agc3RvcC1jb2xvcj0iIzAwMDAiIG9mZnNldD0iNzAlIiAvPgogICAgPC9saW5lYXJHcmFkaWVudD4KICA8L2RlZnM+CiAgPHJlY3Qgd2lkdGg9IjcwMCIgaGVpZ2h0PSI0NzUiIGZpbGw9IiMwMDAwIiAvPgogIDxyZWN0IGlkPSJyIiB3aWR0aD0iNzAwIiBoZWlnaHQ9IjQ3NSIgZmlsbD0idXJsKCNnKSIgLz4KICA8YW5pbWF0ZSB4bGluazpocmVmPSIjciIgYXR0cmlidXRlTmFtZT0ieCIgZnJvbT0iLTcwMCIgdG89IjcwMCIgZHVyPSIxcyIgcmVwZWF0Q291bnQ9ImluZGVmaW5pdGUiICAvPgo8L3N2Zz4=\"}]}],[\"$\",\"figcaption\",null,{\"className\":\"mt-2 text-center text-sm text-gray-600 dark:text-gray-400\",\"dangerouslySetInnerHTML\":{\"__html\":\"Kosmos-2 result for frame 100\"}}]]}],[\"$\",\"figure\",\"3\",{\"className\":\"flex flex-col items-center\",\"children\":[[\"$\",\"div\",null,{\"className\":\"relative overflow-hidden\",\"children\":[\"$\",\"$L11\",null,{\"src\":\"https://cdn.math.computer/v/kosmos2/fish/sm/frame_001000.png\",\"alt\":\"Annotated Frame 1000\",\"width\":960,\"height\":540,\"className\":\"w-max-full\",\"sizes\":\"960px\",\"quality\":78,\"priority\":false,\"loading\":\"lazy\",\"placeholder\":\"blur\",\"blurDataURL\":\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjcwMCIgaGVpZ2h0PSI0NzUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgPGRlZnM+CiAgICA8bGluZWFyR3JhZGllbnQgaWQ9ImciPgogICAgICA8c3RvcCBzdG9wLWNvbG9yPSIjMDAwMCIgb2Zmc2V0PSIyMCUiIC8+CiAgICAgIDxzdG9wIHN0b3AtY29sb3I9IiMwMDAxIiBvZmZzZXQ9IjUwJSIgLz4KICAgICAgPHN0b3Agc3RvcC1jb2xvcj0iIzAwMDAiIG9mZnNldD0iNzAlIiAvPgogICAgPC9saW5lYXJHcmFkaWVudD4KICA8L2RlZnM+CiAgPHJlY3Qgd2lkdGg9IjcwMCIgaGVpZ2h0PSI0NzUiIGZpbGw9IiMwMDAwIiAvPgogIDxyZWN0IGlkPSJyIiB3aWR0aD0iNzAwIiBoZWlnaHQ9IjQ3NSIgZmlsbD0idXJsKCNnKSIgLz4KICA8YW5pbWF0ZSB4bGluazpocmVmPSIjciIgYXR0cmlidXRlTmFtZT0ieCIgZnJvbT0iLTcwMCIgdG89IjcwMCIgZHVyPSIxcyIgcmVwZWF0Q291bnQ9ImluZGVmaW5pdGUiICAvPgo8L3N2Zz4=\"}]}],[\"$\",\"figcaption\",null,{\"className\":\"mt-2 text-center text-sm text-gray-600 dark:text-gray-400\",\"dangerouslySetInnerHTML\":{\"__html\":\"Kosmos-2 result for frame 1000\"}}]]}]]}]]}]}]}],[\"$\",\"nav\",null,{\"className\":\"flex justify-between w-full mt-8 mb-4 text-3xl font-bold text-gray-600 dark:text-gray-400 px-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex-1 flex justify-start\",\"children\":null}],[\"$\",\"div\",null,{\"className\":\"flex-1 flex justify-end\",\"children\":[\"$\",\"$Lf\",null,{\"href\":\"/1/2\",\"className\":\"hover:underline\",\"children\":\"→\"}]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"MathematicalMichaelx2025\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Digital Art Log for Dr. Michael Pilosov\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/2025/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"48x48\"}]]\n3:null\n"])</script></body></html>